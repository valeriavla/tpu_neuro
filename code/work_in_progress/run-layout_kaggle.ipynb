{"cells":[{"cell_type":"markdown","metadata":{"id":"7HUOn3rWazmi"},"source":["# Instructions\n","\n","Make a copy of this notebook, and save with your initials at the end, so that we do not overwrite each other's.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:00.066754Z","iopub.status.busy":"2023-11-16T23:52:00.066374Z","iopub.status.idle":"2023-11-16T23:52:23.775102Z","shell.execute_reply":"2023-11-16T23:52:23.773864Z","shell.execute_reply.started":"2023-11-16T23:52:00.066718Z"},"executionInfo":{"elapsed":98462,"status":"ok","timestamp":1700106283170,"user":{"displayName":"Valeria Vera Lagos","userId":"03700786808723630376"},"user_tz":300},"id":"kBlBXPqUg8Ti","outputId":"13dcc5a5-788c-4b3f-f108-ec9fcb81a822","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow_gnn in /opt/conda/lib/python3.10/site-packages (0.6.0)\n","Requirement already satisfied: google-vizier>=0.0.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (0.1.12)\n","Requirement already satisfied: ml-collections in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (0.1.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (3.1)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (9.0.0)\n","Requirement already satisfied: apache-beam<2.47.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (2.46.0)\n","Requirement already satisfied: tensorflow>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (2.13.0)\n","Requirement already satisfied: protobuf<4,>3.12.2 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.20.3)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.7)\n","Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.9.5)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.3.1.1)\n","Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.2.1)\n","Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.8.2)\n","Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.18)\n","Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.51.1)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.7.2)\n","Requirement already satisfied: httplib2<0.22.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.21.0)\n","Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.24.3)\n","Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.6.1)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.13.0)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.22.3)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.4.2)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.8.2)\n","Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2023.3)\n","Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2023.8.8)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (4.5.0)\n","Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.22.0)\n","Requirement already satisfied: attrs==23.1.0 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (23.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.4.0)\n","Requirement already satisfied: portpicker>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.6.0)\n","Requirement already satisfied: grpcio-tools>=1.35.0 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.48.2)\n","Requirement already satisfied: googleapis-common-protos>=1.56.4 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.60.0)\n","Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.4.20)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (68.1.2)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.16.0)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.3.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.34.0)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml-collections->tensorflow_gnn) (6.0.1)\n","Requirement already satisfied: contextlib2 in /opt/conda/lib/python3.10/site-packages (from ml-collections->tensorflow_gnn) (21.6.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.10.0->tensorflow_gnn) (0.41.2)\n","Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<2.47.0->tensorflow_gnn) (0.6.2)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.22.0,>=0.8->apache-beam<2.47.0->tensorflow_gnn) (3.0.9)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from portpicker>=1.3.1->google-vizier>=0.0.13->tensorflow_gnn) (5.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier>=0.0.13->tensorflow_gnn) (2.0.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (2.22.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (3.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10.0->tensorflow_gnn) (3.2.2)\n","Requirement already satisfied: tensorflow_ranking in /opt/conda/lib/python3.10/site-packages (0.5.3)\n","Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.4.0)\n","Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.24.3)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.16.0)\n","Requirement already satisfied: tensorflow-serving-api<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (2.13.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.51.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.20.3)\n","Requirement already satisfied: tensorflow<3,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.13.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (68.1.2)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.3.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.34.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.22.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.2)\n"]}],"source":["!pip install tensorflow_gnn --pre\n","!pip install tensorflow_ranking"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:23.777985Z","iopub.status.busy":"2023-11-16T23:52:23.777652Z","iopub.status.idle":"2023-11-16T23:52:27.401459Z","shell.execute_reply":"2023-11-16T23:52:27.400520Z","shell.execute_reply.started":"2023-11-16T23:52:23.777948Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["### LAYOUT\n","import collections\n","import functools\n","import hashlib\n","import io\n","import os\n","from typing import NamedTuple\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","import tqdm\n","\n","class FakeBoolFlag(NamedTuple):\n","    value: bool\n","_TOY_DATA = FakeBoolFlag(value=False)\n","\n","\n","\n","class LayoutExample(NamedTuple):\n","  \"\"\"Single example of layout graph.\"\"\"\n","  total_nodes: tf.Tensor  # shape []\n","  total_edges: tf.Tensor  # shape []\n","  total_configs: tf.Tensor  # shape []\n","  total_config_nodes: tf.Tensor  # shape []\n","\n","  node_features: tf.Tensor  # shape [total_nodes, node_feat_size]\n","  node_ops: tf.Tensor  # shape [total_nodes]\n","  edges: tf.Tensor  # shape [total_edges, 2]\n","  # shape[total_configs, total_config_nodes, conf_feat_size]:\n","  node_config_features: tf.Tensor\n","  config_runtimes: tf.Tensor  # shape [total_configs]\n","  argsort_config_runtimes: tf.Tensor  # shape [total_configs]\n","  graph_id: tf.Tensor  # shape []\n","\n","  node_config_ids: tf.Tensor  # shape [total_config_nodes]\n","  node_splits: tf.Tensor\n","\n","  def to_graph_tensor(\n","      self, config_samples: int = -1, max_nodes: int = -1) -> tfgnn.GraphTensor:\n","    \"\"\"Returns `GraphTensor` (sampled if `max(max_nodes, config_samples) >= 0`).\n","\n","    Args:\n","      config_samples: if -1, then all module configurations (and their runtimes)\n","        are returned. If >=0, then this many module configurations (and their\n","        corresponding runtimes) are sampled uniformly at random.\n","      max_nodes: Number of nodes to keep in `\"sampled_feed\"` and\n","        `\"sampled_config\"` edge sets. Regardless, edges for all nodes will be\n","        present in `\"feed\"` and `\"config\"`. If `< 0`, then `\"sampled_config\"`\n","        and `\"config\"` will be identical, also `\"sampled_feed\"` and `\"feed\"`.\n","\n","    Returns:\n","      GraphTensor with node-sets:\n","        + `\"op\"` with features=(\n","            'op': int-vector, 'feats': float-matrix,\n","            'selected': bool-vector indicating if node has edges in\n","                        `\"sampled_*\"` edge-sets).\n","        + `\"nconfigs\"` (\n","            feats='feats': float-tensor with shape\n","              `[num_configurable_nodes, num_configs, config_feat_dim]`).\n","        + `\"g\"` (stands for \"graph\") has one (root) node connecting to all\n","          `\"op\"` and `\"nconfigs\"`.\n","          features=('graph_id': filename of graph (without .npz extension),\n","                    'runtimes': vector of targets with shape `[num_configs]`)\n","      and edge-sets:\n","        + 'feed': directed edges connecting op-node to op-node.\n","        + 'config': edges connecting each `\"nconfig\"` node with a different\n","          `\"op\"` node.\n","        + 'sampled_feed' and 'sampled_config': contain a subset of edges of the\n","          above two. Specifically, ones incident on sampled `op` nodes, for\n","          which feature `selected` is set.\n","        + 'g_op': edges connecting the singleton `\"g\"` node to every `\"op\"` node\n","        + 'g_config': edges connecting the singleton `\"g\"` node to every\n","          `\"nconfig\"` node.\n","    \"\"\"\n","    config_features = self.node_config_features\n","    config_runtimes = self.config_runtimes\n","    num_config_nodes = tf.shape(config_features)[1]\n","    config_node_ids = tf.range(num_config_nodes, dtype=tf.int32)\n","\n","    # If sampling is requested.\n","    if config_samples >= 0:\n","      argsort_config_runtimes = self.argsort_config_runtimes\n","      input_num_configs = tf.shape(self.config_runtimes)[0]\n","      # Skew sampling towards good runtimes.\n","      select_idx = tf.nn.top_k(\n","          # Sample wrt GumbulSoftmax([NumConfs, NumConfs-1, ..., 1])\n","          tf.cast(\n","              (input_num_configs - tf.range(input_num_configs))\n","              /input_num_configs, tf.float32) - tf.math.log(\n","                  -tf.math.log(tf.random.uniform([input_num_configs], 0, 1))),\n","          config_samples)[1]\n","\n","      select_idx = tf.gather(argsort_config_runtimes, select_idx)\n","      # num_configs = config_samples\n","      config_runtimes = tf.gather(config_runtimes, select_idx)\n","      config_features = tf.gather(config_features, select_idx)\n","\n","    ## As we do dropout on (sampled) nodes, maintain a list of edges to keep.\n","    keep_feed_src = full_feed_src = self.edges[:, 0]\n","    keep_feed_tgt = full_feed_tgt = self.edges[:, 1]\n","    keep_config_src = full_config_src = tf.range(\n","        tf.shape(self.node_config_ids)[0])\n","    keep_config_tgt = full_config_tgt = tf.cast(\n","        self.node_config_ids, tf.int32)\n","    op_node_ids = tf.range(self.total_nodes, dtype=tf.int32)\n","    node_is_selected = tf.ones([self.total_nodes], dtype=tf.bool)\n","    kept_node_ratio = tf.ones([], dtype=tf.float32)\n","    node_ops = self.node_ops\n","    node_feats = self.node_features\n","\n","    if max_nodes >= 0:\n","      num_segments = tf.cast(\n","          tf.math.ceil(self.total_nodes / max_nodes), tf.int32)\n","      segment_id = tf.random.uniform(\n","          shape=[], minval=0, maxval=num_segments, dtype=tf.int32)\n","      start_idx = segment_id * max_nodes\n","      end_idx = (segment_id + 1) * max_nodes\n","      end_idx = tf.minimum(end_idx, self.total_nodes)\n","      node_is_selected = tf.logical_and(\n","          op_node_ids >= start_idx, op_node_ids < end_idx)\n","\n","      feed_edge_mask = tf.logical_and(\n","          self.edges >= start_idx, self.edges < end_idx)\n","      feed_edge_mask = tf.logical_and(\n","          feed_edge_mask[:, 0], feed_edge_mask[:, 1])\n","      config_edge_mask = tf.logical_and(\n","          full_config_tgt >= start_idx, full_config_tgt < end_idx)\n","\n","      kept_node_ratio = tf.cast((end_idx - start_idx) / self.total_nodes,\n","                                tf.float32)\n","\n","      keep_feed_src = tf.boolean_mask(full_feed_src, feed_edge_mask)\n","      keep_feed_tgt = tf.boolean_mask(full_feed_tgt, feed_edge_mask)\n","\n","      keep_config_src = tf.boolean_mask(full_config_src, config_edge_mask)\n","      keep_config_tgt = tf.boolean_mask(full_config_tgt, config_edge_mask)\n","\n","    return tfgnn.GraphTensor.from_pieces(\n","        node_sets={\n","            'op': tfgnn.NodeSet.from_fields(\n","                sizes=tf.shape(op_node_ids),\n","                features={\n","                    'op': node_ops,\n","                    'feats': node_feats,\n","                    'selected': node_is_selected,\n","                }\n","            ),\n","            'nconfig': tfgnn.NodeSet.from_fields(  # Node-specific configs.\n","                features={\n","                    'feats': tf.transpose(config_features, [1, 0, 2]),\n","                },\n","                sizes=tf.shape(self.node_config_ids),\n","            ),\n","            'g': tfgnn.NodeSet.from_fields(\n","                features={\n","                    'graph_id': tf.expand_dims(self.graph_id, 0),\n","                    'runtimes': tf.expand_dims(config_runtimes, 0),\n","                    'kept_node_ratio': tf.expand_dims(kept_node_ratio, 0),\n","                },\n","                sizes=tf.constant([1]))\n","        },\n","        edge_sets={\n","            'config': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(full_config_src),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('nconfig', full_config_src),\n","                    target=('op', full_config_tgt))),\n","            'feed': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(full_feed_src),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('op', full_feed_src),\n","                    target=('op', full_feed_tgt))),\n","            'g_op': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(op_node_ids),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('g', tf.zeros_like(op_node_ids)),\n","                    target=('op', op_node_ids))),\n","            'g_config': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(config_node_ids),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('g', tf.zeros_like(config_node_ids)),\n","                    target=('nconfig', config_node_ids))),\n","            'sampled_config': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(keep_config_src),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('nconfig', keep_config_src),\n","                    target=('op', keep_config_tgt))),\n","            'sampled_feed': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.shape(keep_feed_src),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('op', keep_feed_src),\n","                    target=('op', keep_feed_tgt))),\n","        })\n","\n","\n","class NpzDatasetPartition:\n","  \"\"\"Holds one data partition (train, test, validation) on device memory.\"\"\"\n","\n","  def __init__(self):\n","    # Populated in `add_npz_file()`.\n","    self._data_dict: dict[str, list[np.ndarray]] = collections.defaultdict(list)\n","    self._num_edges: list[int] = [0]    # prepend with 0 to prep for cumsum.\n","    self._num_configs: list[int] = [0]  # ^^\n","    self._num_nodes: list[int] = [0]    # ^^\n","    self._num_config_nodes: list[int] = [0]  # ^^\n","    self._num_node_splits: list[int] = [0]   # ^^\n","\n","    # Populated in `finalize()`.\n","    self.node_feat: 'tf.Tensor | None' = None   # indexed by node_ranges.\n","    self.node_opcode: 'tf.Tensor | None' = None  # ^^\n","    self.edge_index: 'tf.Tensor | None' = None   # indexed by edge_ranges.\n","    self.config_runtime: 'tf.Tensor | None' = None  # indexed by config_ranges.\n","    self.argsort_config_runtime: tf.Tensor|None = None  # by flat_config_ranges.\n","    self.graph_id: 'tf.Tensor | None' = None\n","    # indexed by config_ranges and config_node_ranges\n","    self.node_config_feat: 'tf.Tensor | None' = None\n","\n","    # finalize() sets to: cumsum([0, numEdges(graph_1), numEdges(graph_2), ..]).\n","    self.edge_ranges: 'tf.Tensor | None' = None\n","    # finalize() sets to: cumsum([0, numNodes(graph_1), numNodes(graph_2), ..]).\n","    self.node_ranges: 'tf.Tensor | None' = None\n","    # finalize() sets to: cumsum([0, numConfigs(graph_1), nCfgs(graph_2), ..]).\n","    self.config_ranges: 'tf.Tensor | None' = None\n","    # finalize() sets to: cumsum([0, numModules(graph_1), nModul(graph_2), ..]).\n","    self.config_node_ranges: 'tf.Tensor | None' = None\n","    # _compute_flat_config_ranges (via finalize() and load_from_file()) sets to:\n","    # cumsum([0, numConfigs(graph_1) * numModules(graph_1), ... ])\n","    self.flat_config_ranges: 'tf.Tensor | None' = None\n","\n","    self.node_split_ranges: 'tf.Tensor | None' = None\n","    self.node_splits: 'tf.Tensor | None' = None\n","    self.node_config_ids: 'tf.Tensor | None' = None\n","\n","  def save_to_file(self, cache_file: str):\n","    \"\"\"Saves dataset as numpy. Can be restored with `load_from_file`.\"\"\"\n","    print('Saving ...')\n","    assert self.node_feat is not None, 'finalize() was not invoked'\n","    assert self.node_opcode is not None\n","    assert self.edge_index is not None\n","    assert self.node_config_feat is not None\n","    assert self.config_runtime is not None\n","    assert self.argsort_config_runtime is not None\n","    assert self.node_splits is not None\n","    assert self.node_config_ids is not None\n","\n","    assert self.graph_id is not None\n","    assert self.edge_ranges is not None\n","    assert self.node_ranges is not None\n","    assert self.config_ranges is not None\n","    assert self.config_node_ranges is not None\n","    assert self.node_split_ranges is not None\n","    assert self.flat_config_ranges is not None\n","\n","    np_dict = dict(\n","        node_feat=self.node_feat.numpy(),\n","        node_opcode=self.node_opcode.numpy(),\n","        edge_index=self.edge_index.numpy(),\n","        node_config_feat=self.node_config_feat.numpy(),\n","        config_runtime=self.config_runtime.numpy(),\n","        argsort_config_runtime=self.argsort_config_runtime.numpy(),\n","        edge_ranges=self.edge_ranges.numpy(),\n","        node_ranges=self.node_ranges.numpy(),\n","        config_ranges=self.config_ranges.numpy(),\n","        node_split_ranges=self.node_split_ranges.numpy(),\n","        flat_config_ranges=self.flat_config_ranges.numpy(),\n","        config_node_ranges=self.config_node_ranges.numpy(),\n","        node_splits=self.node_splits.numpy(),\n","        node_config_ids=self.node_config_ids.numpy(),\n","    )\n","    bytes_io = io.BytesIO()\n","    np.savez_compressed(bytes_io, **np_dict)\n","    with tf.io.gfile.GFile(cache_file, 'wb') as fout:\n","      fout.write(bytes_io.getvalue())\n","    print('wrote ' + cache_file)\n","    graph_ids_file = cache_file + '.graphs.txt'\n","    with tf.io.gfile.GFile(graph_ids_file, 'w') as fout:\n","      fout.write(b'\\n'.join(self.graph_id.numpy().tolist()).decode())\n","    print('wrote ' + graph_ids_file)\n","\n","  def load_from_file(self, cache_file: str):\n","    \"\"\"Loads dataset from numpy file.\"\"\"\n","    np_dict = np.load(tf.io.gfile.GFile(cache_file, 'rb'))\n","    self.node_feat = tf.constant(np_dict['node_feat'])\n","    self.node_opcode = tf.constant(np_dict['node_opcode'])\n","    self.edge_index = tf.constant(np_dict['edge_index'])\n","    self.node_config_feat = tf.constant(np_dict['node_config_feat'])\n","    self.config_runtime = tf.constant(np_dict['config_runtime'])\n","    self.argsort_config_runtime = tf.constant(np_dict['argsort_config_runtime'])\n","    self.edge_ranges = tf.constant(np_dict['edge_ranges'])\n","    self.node_ranges = tf.constant(np_dict['node_ranges'])\n","    self.config_ranges = tf.constant(np_dict['config_ranges'])\n","    self.config_node_ranges = tf.constant(np_dict['config_node_ranges'])\n","    self.node_splits = tf.constant(np_dict['node_splits'])\n","    self.node_config_ids = tf.constant(np_dict['node_config_ids'])\n","    self.node_split_ranges = tf.constant(np_dict['node_split_ranges'])\n","    self.flat_config_ranges = tf.constant(np_dict['flat_config_ranges'])\n","    graph_ids = tf.io.gfile.GFile(cache_file + '.graphs.txt', 'r').readlines()\n","    self.graph_id = tf.stack([graph_id.rstrip() for graph_id in graph_ids])\n","    self._compute_flat_config_ranges()\n","    print('loaded from ' + cache_file)\n","\n","  def add_npz_file(\n","      self, graph_id: str, npz_file: np.lib.npyio.NpzFile,\n","      min_configs: int = 2, max_configs=-1):\n","    \"\"\"Copies data from npz file into this class instance.\n","\n","    After finishing all calls `add_npz_file()`, user must invoke `finalize()`.\n","\n","    Args:\n","      graph_id: the filename (without extension) that npz_file was read from.\n","      npz_file: Output of np.load on a file from the TpuGraphs Tiles dataset.\n","      min_configs: The file be incorporated only if the number of module\n","        configurations is equal or greater than this.\n","      max_configs: If >0, only this many configurations will be sampled for the\n","        graph.\n","    \"\"\"\n","    npz_data = dict(npz_file.items())\n","    num_configs = npz_data['node_config_feat'].shape[0]\n","    num_config_nodes = npz_data['node_config_feat'].shape[1]\n","    assert npz_data['node_config_feat'].shape[2] == 18\n","    npz_data['node_splits'] = npz_data['node_splits'].reshape([-1])\n","    npz_data['argsort_config_runtime'] = np.argsort(npz_data['config_runtime'])\n","    if num_configs < min_configs:\n","      print('skipping graph with only %i configurations' % num_configs)\n","      return\n","    if max_configs > 0 and num_configs > max_configs:\n","      third = max_configs // 3\n","      keep_indices = np.concatenate([\n","          npz_data['argsort_config_runtime'][:third],  # Good configs.\n","          npz_data['argsort_config_runtime'][-third:],  # Bad configs.\n","          np.random.choice(\n","              npz_data['argsort_config_runtime'][third:-third],\n","              max_configs - 2 * third)\n","      ], axis=0)\n","      num_configs = max_configs\n","      npz_data['node_config_feat'] = npz_data['node_config_feat'][keep_indices]\n","      npz_data['config_runtime'] = npz_data['config_runtime'][keep_indices]\n","      npz_data['argsort_config_runtime'] = np.argsort(  # re-sort.\n","          npz_data['config_runtime'])\n","    npz_data['node_config_feat'] = npz_data['node_config_feat'].reshape(\n","        (num_configs * num_config_nodes, -1))\n","    for key, ndarray in npz_data.items():\n","      self._data_dict[key].append(ndarray)\n","    self._data_dict['graph_id'].append(np.array(graph_id))\n","    num_nodes = npz_data['node_feat'].shape[0]\n","    num_edges = npz_data['edge_index'].shape[0]\n","    assert num_config_nodes == npz_data['node_config_ids'].shape[0]\n","    assert num_nodes == npz_data['node_opcode'].shape[0]\n","    assert num_configs == npz_data['config_runtime'].shape[0]\n","    self._num_nodes.append(num_nodes)\n","    self._num_config_nodes.append(num_config_nodes)\n","    self._num_node_splits.append(npz_data['node_splits'].shape[0])\n","    self._num_edges.append(num_edges)\n","    self._num_configs.append(num_configs)\n","\n","  def finalize(self):\n","    \"\"\"Combines the list of dicts to contiguous tensors (by concat or stack).\n","\n","    Afterwards, caller is able to call `get_item()` on this class instance.\n","    \"\"\"\n","    self.graph_id = tf.stack(self._data_dict.pop('graph_id'), axis=0)\n","    self.node_feat = tf.concat(self._data_dict.pop('node_feat'), axis=0)\n","    self.node_opcode = tf.concat(self._data_dict.pop('node_opcode'), axis=0)\n","    self.edge_index = tf.concat(self._data_dict.pop('edge_index'), axis=0)\n","\n","    self.node_config_feat = tf.concat(\n","        self._data_dict.pop('node_config_feat'), axis=0)\n","    self.config_runtime = tf.concat(\n","        self._data_dict.pop('config_runtime'), axis=0)\n","    self.argsort_config_runtime = tf.concat(\n","        self._data_dict.pop('argsort_config_runtime'), axis=0)\n","    self.node_config_ids = tf.concat(\n","        self._data_dict.pop('node_config_ids'), axis=0)\n","    self.node_splits = tf.concat(self._data_dict.pop('node_splits'), axis=0)\n","\n","    self.edge_ranges = tf.cumsum(self._num_edges)\n","    self.node_ranges = tf.cumsum(self._num_nodes)\n","    self.config_node_ranges = tf.cumsum(self._num_config_nodes)\n","    self.config_ranges = tf.cumsum(self._num_configs)\n","    self.node_split_ranges = tf.cumsum(self._num_node_splits)\n","    self._compute_flat_config_ranges()\n","\n","  def _compute_flat_config_ranges(self):\n","    num_configs = tf.cast(  # undo cumsum.\n","        self.config_ranges[1:] - self.config_ranges[:-1], tf.int64)\n","    num_config_nodes = tf.cast(  # undo cumsum.\n","        self.config_node_ranges[1:] - self.config_node_ranges[:-1], tf.int64)\n","    self.flat_config_ranges = tf.cumsum(\n","        tf.concat(\n","            [tf.zeros([1], dtype=tf.int64), num_configs * num_config_nodes],\n","            axis=0))\n","\n","  def get_item(self, index: int) -> LayoutExample:\n","    \"\"\"Returns `LayoutExample` encoding graph (by order of `add_npz_file`).\"\"\"\n","    node_start = self.node_ranges[index]\n","    node_end = self.node_ranges[index + 1]\n","    edge_start = self.edge_ranges[index]\n","    edge_end = self.edge_ranges[index + 1]\n","    config_start = self.config_ranges[index]\n","    config_end = self.config_ranges[index + 1]\n","    config_node_start = self.config_node_ranges[index]\n","    config_node_end = self.config_node_ranges[index + 1]\n","    flat_config_start = self.flat_config_ranges[index]\n","    flat_config_end = self.flat_config_ranges[index + 1]\n","    node_split_start = self.node_split_ranges[index]\n","    node_split_end = self.node_split_ranges[index + 1]\n","\n","    num_configs = config_end - config_start\n","    num_config_nodes = config_node_end - config_node_start\n","\n","    flat_config = self.node_config_feat[flat_config_start:flat_config_end]\n","    config_tensor = tf.reshape(\n","        flat_config, [num_configs, num_config_nodes, flat_config.shape[-1]])\n","\n","    return LayoutExample(\n","        node_features=self.node_feat[node_start:node_end],\n","        node_ops=self.node_opcode[node_start:node_end],\n","        edges=tf.cast(self.edge_index[edge_start:edge_end], tf.int32),\n","        node_config_features=config_tensor,\n","        node_config_ids=tf.cast(\n","            self.node_config_ids[config_node_start:config_node_end], tf.int32),\n","        node_splits=self.node_splits[node_split_start:node_split_end],\n","        config_runtimes=self.config_runtime[config_start:config_end],\n","        argsort_config_runtimes=(\n","            self.argsort_config_runtime[config_start:config_end]),\n","        graph_id=self.graph_id[index],\n","        total_nodes=node_end - node_start,\n","        total_edges=edge_end - edge_start,\n","        total_configs=config_end - config_start,\n","        total_config_nodes=config_node_end - config_node_start)\n","\n","  def get_graph_tensors_dataset(\n","      self, config_samples: int, max_nodes: int = -1) -> tf.data.Dataset:\n","    if self.edge_ranges is None:\n","      raise ValueError('finalize() was not invoked.')\n","    dataset = tf.data.Dataset.range(self.edge_ranges.shape[0] - 1)\n","    dataset = dataset.map(self.get_item, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.map(\n","        functools.partial(LayoutExample.to_graph_tensor,\n","                          config_samples=config_samples, max_nodes=max_nodes))\n","    return dataset\n","\n","  def iter_graph_tensors(self):\n","    if self.edge_ranges is None:\n","      raise ValueError('finalize() was not invoked.')\n","    assert self.edge_ranges is not None\n","    for i in range(self.edge_ranges.shape[0] - 1):\n","      yield self.get_item(i).to_graph_tensor()\n","\n","\n","class NpzDataset(NamedTuple):\n","  \"\"\"Contains all partitions of the dataset.\"\"\"\n","  train: NpzDatasetPartition\n","  validation: NpzDatasetPartition\n","  test: NpzDatasetPartition\n","\n","  @property\n","  def num_ops(self):\n","    return int(\n","        tf.reduce_max([\n","            tf.reduce_max(self.train.node_opcode),\n","            tf.reduce_max(self.validation.node_opcode),\n","            tf.reduce_max(self.test.node_opcode),\n","        ]).numpy()) + 1\n","\n","  def _get_normalizer(self, feature_matrix) -> tuple[\n","      tf.Tensor, tf.Tensor, tf.Tensor]:\n","    max_feat = tf.reduce_max(feature_matrix, axis=0, keepdims=True)\n","    min_feat = tf.reduce_min(feature_matrix, axis=0, keepdims=True)\n","    return min_feat[0] != max_feat[0], min_feat, max_feat\n","\n","  def _apply_normalizer(self, feature_matrix, used_columns, min_feat, max_feat):\n","    feature_matrix = tf.boolean_mask(feature_matrix, used_columns, axis=1)\n","    min_feat = tf.boolean_mask(min_feat, used_columns, axis=1)\n","    max_feat = tf.boolean_mask(max_feat, used_columns, axis=1)\n","    return (feature_matrix - min_feat) / (max_feat - min_feat)\n","\n","  def normalize(self):\n","    \"\"\"Removes constant features and normalizes remaining onto [0, 1].\n","\n","    The statistics are computed only from train partition then applied to all\n","    partitions {train, test, validation}.\n","    \"\"\"\n","    normalizer_args = self._get_normalizer(self.train.node_feat)\n","    self.train.node_feat = self._apply_normalizer(\n","        self.train.node_feat, *normalizer_args)\n","    self.validation.node_feat = self._apply_normalizer(\n","        self.validation.node_feat, *normalizer_args)\n","    self.test.node_feat = self._apply_normalizer(\n","        self.test.node_feat, *normalizer_args)\n","\n","    normalizer_args = self._get_normalizer(self.train.node_config_feat)\n","    self.train.node_config_feat = self._apply_normalizer(\n","        self.train.node_config_feat, *normalizer_args)\n","    self.validation.node_config_feat = self._apply_normalizer(\n","        self.validation.node_config_feat, *normalizer_args)\n","    self.test.node_config_feat = self._apply_normalizer(\n","        self.test.node_config_feat, *normalizer_args)\n","\n","\n","def get_npz_split(\n","    split_path: str, min_configs=2, max_configs=-1,\n","    cache_dir=None) -> NpzDatasetPartition:\n","  \"\"\"Returns data for a single partition.\"\"\"\n","  glob_pattern = os.path.join(split_path, '*.npz')\n","  files = tf.io.gfile.glob(glob_pattern)\n","  if not files:\n","    raise ValueError('No files matched: ' + glob_pattern)\n","  if _TOY_DATA.value:\n","    files = files[:5]\n","\n","  cache_filename = None\n","  if cache_dir:\n","    if not tf.io.gfile.exists(cache_dir):\n","      tf.io.gfile.makedirs(cache_dir)\n","    filename_hash = hashlib.md5(\n","        f'{split_path}:{min_configs}:{max_configs}:{_TOY_DATA.value}'.encode()\n","        ).hexdigest()\n","    cache_filename = os.path.join(cache_dir, f'{filename_hash}-cache.npz')\n","    print('dataset cache file: ', cache_filename)\n","\n","  npz_dataset = NpzDatasetPartition()\n","  if cache_filename and tf.io.gfile.exists(cache_filename):\n","    npz_dataset.load_from_file(cache_filename)\n","  else:\n","    for filename in tqdm.tqdm(files):\n","      np_data = np.load(tf.io.gfile.GFile(filename, 'rb'))\n","      graph_id = os.path.splitext(os.path.basename(filename))[0]\n","      npz_dataset.add_npz_file(\n","          graph_id, np_data, min_configs=min_configs, max_configs=max_configs)\n","    npz_dataset.finalize()\n","    if cache_filename:\n","      npz_dataset.save_to_file(cache_filename)\n","\n","  return npz_dataset\n","\n","\n","def get_npz_dataset(\n","    root_path: str, min_train_configs=-1, max_train_configs=-1,\n","    cache_dir: 'None | str' = None) -> NpzDataset:\n","  \"\"\"Returns {train, test, validation} partitions of layout dataset collection.\n","\n","  All partitions will be normalized: statistics are computed from training set\n","  partition and applied to all partitions.\n","\n","  Args:\n","    root_path: Path where dataset lives. It must have subdirectories 'train',\n","      'test' and 'valid'.\n","    min_train_configs: If > 0, then layout examples will be filtered to have at\n","      least this many configurations (features and runtimes).\n","    max_train_configs: Training and validation graphs will be truncated to\n","      include only this many configurations. Set this according to your\n","      available device memory. If you have lots of memory, you may set to -1,\n","      to include all configurations for all {train, validation} graphs.\n","    cache_dir: If given, the many files for each of {train, test, validation}\n","      will be stored as one file (makes loading faster, for future runs).\n","  \"\"\"\n","  npz_dataset = NpzDataset(\n","      train=get_npz_split(\n","          os.path.join(root_path, 'train'), cache_dir=cache_dir,\n","          min_configs=min_train_configs, max_configs=max_train_configs),\n","      validation=get_npz_split(\n","          os.path.join(root_path, 'valid'), cache_dir=cache_dir,\n","          min_configs=min_train_configs, max_configs=max_train_configs),\n","      test=get_npz_split(\n","          os.path.join(root_path, 'test'), cache_dir=cache_dir))\n","  npz_dataset.normalize()\n","  return npz_dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.403245Z","iopub.status.busy":"2023-11-16T23:52:27.402763Z","iopub.status.idle":"2023-11-16T23:52:27.443507Z","shell.execute_reply":"2023-11-16T23:52:27.442701Z","shell.execute_reply.started":"2023-11-16T23:52:27.403218Z"},"trusted":true},"outputs":[],"source":["###IMPLICIT\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","\n","EPSILON = 1e-6  # To prevent division by 0.\n","\n","\n","class Multiplier:\n","  \"\"\"Holds an (implicit) matrix that can be multiplied with dense matrices.\"\"\"\n","  _transpose: 'Multiplier' = None\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    raise NotImplementedError()\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    raise NotImplementedError()\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    raise NotImplementedError()\n","\n","  def __matmul__(self, mat: tf.Tensor) -> tf.Tensor:\n","    tf.assert_equal(self.shape[1], shape(mat)[0])\n","    return self.matmul(mat)\n","\n","  def __rmatmul__(self, mat: tf.Tensor) -> tf.Tensor:\n","    tf.assert_equal(shape(mat)[-1], self.shape[0])\n","    return self.rmatmul(mat)\n","\n","  def __add__(self, mat: 'Multiplier') -> 'Multiplier':\n","    return Sum(self, mat)\n","\n","  def transpose(self) -> 'Multiplier':\n","    if self._transpose is None:\n","      self._transpose = Transpose(self)\n","    return self._transpose\n","\n","  def add_eye(self, diag_weight=float(1.0)) -> 'Multiplier':\n","    tf.assert_equal(self.shape[0], self.shape[1])\n","    return Sum(self, DiagMatrix(diag_weight * tf.ones([self.shape[0]])))\n","\n","  def rowsums(self, replace_if_0: 'None|float|tf.Tensor' = None) -> tf.Tensor:\n","    \"\"\"Returns vector with shape `num_rows = [self.shape[0]]` that sums rows.\n","\n","    Args:\n","      replace_if_0: If None, returns the actual sum, leaving zero-entries as-is.\n","        Otherwise, zero-entries will be replaced by this value.\n","    \"\"\"\n","    y = self @ tf.ones([self.shape[1]])  # M . 1\n","\n","    if replace_if_0 is not None:\n","      y = tf.where(tf.abs(y) < EPSILON, replace_if_0 * tf.ones_like(y), y)\n","    return y\n","\n","  def colsums(self, replace_if_0: 'None|float|tf.Tensor' = None) -> tf.Tensor:\n","    \"\"\"Returns vector with shape `num_cols = [self.shape[1]]` that sums columns.\n","\n","    Args:\n","      replace_if_0: If None, returns the actual sum, leaving zero-entries as-is.\n","        Otherwise, zero-entries will be replaced by this value.\n","    \"\"\"\n","    y = tf.ones([self.shape[0]]) @ self  # 1^T M  [shape=[cols]]\n","\n","    if replace_if_0 is not None:\n","      y = tf.where(tf.abs(y) < EPSILON, replace_if_0 * tf.ones_like(y), y)\n","    return y\n","\n","  def normalize_left(self) -> 'Multiplier':\n","    \"\"\"Returns a left-stochastic matrix.\"\"\"\n","    return Product(self, DiagMatrix(1 / self.colsums(1.0)))\n","\n","  def normalize_right(self) -> 'Multiplier':\n","    \"\"\"Returns a right-stochastic matrix.\"\"\"\n","    return Product(DiagMatrix(1 / self.rowsums(1.0)), self)\n","\n","  def normalize_leftright(self) -> 'Multiplier':\n","    return Product(\n","        DiagMatrix(tf.math.rsqrt(self.rowsums(1.0))),\n","        self,\n","        DiagMatrix(tf.math.rsqrt(self.colsums(1.0))),\n","    )\n","\n","  def normalize_symmetric(self) -> 'Multiplier':\n","    inv_sqrt_degree = DiagMatrix(tf.math.rsqrt(self.colsums(1.0)))\n","    return Product(inv_sqrt_degree, self, inv_sqrt_degree)\n","\n","\n","class Transpose(Multiplier):\n","  \"\"\"Defines matrix transpose.\"\"\"\n","\n","  def __init__(self, multiplier: Multiplier):\n","    self._multiplier = multiplier\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    # (M'X) == (X'M)'\n","    return tf.transpose(tf.transpose(mat) @ self._multiplier)\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    # (XM') == (XM')'' == (M X')'\n","    return tf.transpose(self._multiplier @ tf.transpose(mat))\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    transpose_shape = self._multiplier.shape\n","    return (transpose_shape[1], transpose_shape[0])\n","\n","  def transpose(self) -> Multiplier:\n","    return self._multiplier\n","\n","\n","class DiagMatrix(Multiplier):\n","  \"\"\"Defines diagonal matrix.\"\"\"\n","\n","  def __init__(self, diag_vector: tf.Tensor):\n","    assert len(diag_vector.shape) == 1, 'Must be a vector.'\n","    self._diag_vector = diag_vector\n","    self._vec_shape = shape(diag_vector)[0]\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.einsum('i,i...->i...', self._diag_vector, mat)\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.einsum('i,...i->...i', self._diag_vector, mat)\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return (self._vec_shape, self._vec_shape)\n","\n","\n","class Product(Multiplier):\n","  \"\"\"Defines product of multipliers.\"\"\"\n","\n","  def __init__(self, *multipliers: Multiplier):\n","    assert multipliers\n","    for i in range(1, len(multipliers)):\n","      tf.assert_equal(multipliers[i - 1].shape[1], multipliers[i].shape[0])\n","\n","    self._multipliers = multipliers\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    for m in self._multipliers[::-1]:\n","      mat = m @ mat\n","    return mat\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    for m in self._multipliers:\n","      mat = mat @ m\n","    return mat\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return (self._multipliers[0].shape[0], self._multipliers[-1].shape[1])\n","\n","\n","class Sum(Multiplier):\n","  \"\"\"Defines sum of multipliers.\"\"\"\n","\n","  def __init__(self, *multipliers: Multiplier):\n","    assert multipliers\n","    for i in range(1, len(multipliers)):\n","      tf.assert_equal(multipliers[i].shape[0], multipliers[0].shape[0])\n","      tf.assert_equal(multipliers[i].shape[1], multipliers[0].shape[1])\n","    self._multipliers = multipliers\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.add_n([m @ mat for m in self._multipliers])\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.add_n([mat @ m for m in self._multipliers])\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return self._multipliers[0].shape\n","\n","\n","class AdjacencyMultiplier(Multiplier):\n","  r\"\"\"Multiplies (sparse) adjacency with dense matrices.\n","\n","  Yields adjacency with (rows, cols) == (target, source).\n","\n","  `adj_multiplier @ x` yields tensor `y` with `y[i]` being `\\sum_{j->i} x[j]`.\n","\n","  Init Args:\n","      graph:\n","      sender_tag: If `== tfgnn.SOURCE`, then the (implicit) adjacency will be\n","        of shape `size_target x size_source`. If `== tfgnn.TARGET`, then `shape`\n","        should be `size_source x size_target`.\n","  \"\"\"\n","\n","  def __init__(\n","      self, graph, edge_set_name: tfgnn.EdgeSetName,\n","      edge_weight_feature_name: 'None|tfgnn.FieldName' = None,\n","      sender_tag: tfgnn.IncidentNodeTag = tfgnn.SOURCE):\n","    tfgnn.check_scalar_graph_tensor(graph, 'AdjacencyMultiplier')\n","    self._sender_tag = sender_tag\n","    self._receiver_tag: tfgnn.IncidentNodeTag = 1 - sender_tag\n","    self._edge_set_name = edge_set_name\n","    self._graph = graph\n","    self._edge_weight_feature_name = edge_weight_feature_name\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    \"\"\"Shape is (size of receiver node set, size of sender node set).\"\"\"\n","    adj = self._graph.edge_sets[self._edge_set_name].adjacency\n","    sender_node_set_name = adj.node_set_name(self._sender_tag)\n","    receiver_node_set_name = adj.node_set_name(self._receiver_tag)\n","    sender_sizes = self._graph.node_sets[sender_node_set_name].sizes\n","    receiver_sizes = self._graph.node_sets[receiver_node_set_name].sizes\n","    return (tf.cast(tf.reduce_sum(receiver_sizes), tf.int32),\n","            tf.cast(tf.reduce_sum(sender_sizes), tf.int32))\n","\n","  def matmul(self, mat: tf.Tensor):\n","    edge_level = tfgnn.broadcast_node_to_edges(\n","        self._graph, self._edge_set_name, self._sender_tag, feature_value=mat)\n","\n","    if self._edge_weight_feature_name:\n","      edge_set = self._graph.edge_sets[self._edge_set_name]\n","      edge_level *= edge_set[self._edge_weight_feature_name]\n","\n","    return tfgnn.pool_edges_to_node(\n","        self._graph, self._edge_set_name, self._receiver_tag,\n","        feature_value=edge_level)\n","\n","  def rmatmul(self, mat):\n","    edge_level = tfgnn.broadcast_node_to_edges(\n","        self._graph, self._edge_set_name, self._receiver_tag,\n","        feature_value=tf.transpose(mat))\n","\n","    if self._edge_weight_feature_name:\n","      edge_set = self._graph.edge_sets[self._edge_set_name]\n","      edge_level *= edge_set[self._edge_weight_feature_name]\n","\n","    return tf.transpose(tfgnn.pool_edges_to_node(\n","        self._graph, self._edge_set_name, self._sender_tag,\n","        feature_value=edge_level))\n","\n","\n","def shape(tensor: tf.Tensor) -> 'list[int]|tf.Tensor':\n","  \"\"\"Helper function returns shape of eager or symbolic tensors.\"\"\"\n","  if any([s is None for s in tensor.shape]):\n","    return tf.shape(tensor)\n","  else:\n","    return tensor.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.446779Z","iopub.status.busy":"2023-11-16T23:52:27.445750Z","iopub.status.idle":"2023-11-16T23:52:27.600909Z","shell.execute_reply":"2023-11-16T23:52:27.600011Z","shell.execute_reply.started":"2023-11-16T23:52:27.446742Z"},"id":"u690dfq_hVqs","trusted":true},"outputs":[],"source":["###TRAINING\n","# Install standard modules\n","\n","\n","class _OpEmbedding(tf.keras.Model):\n","  \"\"\"Embeds GraphTensor.node_sets['op']['op'] nodes into feature 'op_e'.\"\"\"\n","\n","  def __init__(self, num_ops: int, embed_d: int, l2reg: float = 1e-4):\n","    super().__init__()\n","    self.embedding_layer = tf.keras.layers.Embedding(\n","        num_ops, embed_d, activity_regularizer=tf.keras.regularizers.l2(l2reg))\n","\n","  def call(\n","      self, graph: tfgnn.GraphTensor,\n","      training: bool = False) -> tfgnn.GraphTensor:\n","    op_features = dict(graph.node_sets['op'].features)\n","    op_features['op_e'] = self.embedding_layer(\n","        tf.cast(graph.node_sets['op']['op'], tf.int32))\n","    return graph.replace_features(node_sets={'op': op_features})\n","\n","\n","def pair_layout_graph_with_label(graph: tfgnn.GraphTensor):\n","    \"\"\"Extracts label from graph (`tfgnn.GraphTensor`) and returns a pair of `(graph, label)`\"\"\"\n","    # Return runtimes divded over large number: only ranking is required. The\n","    # runtimes are in the 100K range\n","    label = tf.cast(graph.node_sets['g']['runtimes'], tf.float32) / 1e7\n","    return graph, label\n","\n","\n","\n","class ResModel(tf.keras.Model):\n","    \"\"\"GNN with residual connections.\"\"\"\n","\n","    def __init__(\n","        self, num_configs: int, num_ops: int, op_embed_dim: int = 32,\n","        num_gnns: int = 2, mlp_layers: int = 2,\n","        hidden_activation: str = 'leaky_relu',\n","        hidden_dim: int = 32, reduction: str = 'sum'):\n","        super().__init__()\n","        self._num_configs = num_configs\n","        self._num_ops = num_ops\n","        self._op_embedding = _OpEmbedding(num_ops, op_embed_dim)\n","        self._prenet = _mlp([hidden_dim] * mlp_layers, hidden_activation)\n","        self._gc_layers = []\n","        for _ in range(num_gnns):\n","            self._gc_layers.append(_mlp([hidden_dim] * mlp_layers, hidden_activation))\n","        self._postnet = _mlp([hidden_dim, 1], hidden_activation, use_bias=False)\n","\n","    def call(self, graph: tfgnn.GraphTensor, training: bool = False):\n","        del training\n","        return self.forward(graph, self._num_configs)\n","\n","    def _node_level_forward(\n","        self, node_features: tf.Tensor,\n","        config_features: tf.Tensor,\n","        graph: tfgnn.GraphTensor, num_configs: int,\n","        edgeset_prefix='') -> tf.Tensor:\n","        \"\"\"implements the full computation within a GNN layer:\n","        obtains adjacency Matrices and normalizes them, \n","        transforms and normalizes nodes and configuration.\n","        applies the Pre-processing MLP and performs the Graph Convolution Operation.\n","        \"\"\"\n","    \n","        adj_op_op = AdjacencyMultiplier(\n","            graph, edgeset_prefix+'feed')  # op->op\n","        adj_config = AdjacencyMultiplier(\n","            graph, edgeset_prefix+'config')  # nconfig->op\n","\n","        adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n","        adj_op_op_hat = adj_op_op_hat.normalize_symmetric()\n","\n","        x = node_features\n","\n","        x = tf.stack([x] * num_configs, axis=1)\n","        config_features = 100 * (adj_config @ config_features)\n","        x = tf.concat([config_features, x], axis=-1)\n","        x = self._prenet(x)\n","        x = tf.nn.leaky_relu(x)\n","\n","        for layer in self._gc_layers:\n","            y = x\n","            y = tf.concat([config_features, y], axis=-1)\n","            y = tf.nn.leaky_relu(layer(adj_op_op_hat @ y))\n","            x += y\n","        return x\n","\n","    def forward(\n","        self, graph: tfgnn.GraphTensor, num_configs: int,\n","        backprop=True) -> tf.Tensor:\n","        \"\"\"\n","        Overall forward pass within the embedding layer,\n","        the node-level forward pass (_node_level_forward),\n","        and the final global pooling and post-processing stages.\n","        \"\"\"\n","        graph = self._op_embedding(graph)\n","\n","        config_features = graph.node_sets['nconfig']['feats']\n","        node_features = tf.concat([\n","            graph.node_sets['op']['feats'],\n","            graph.node_sets['op']['op_e']\n","        ], axis=-1)\n","\n","        x_full = self._node_level_forward(\n","            node_features=tf.stop_gradient(node_features),\n","            config_features=tf.stop_gradient(config_features),\n","            graph=graph, num_configs=num_configs)\n","\n","        if backprop:\n","            x_backprop = self._node_level_forward(\n","                node_features=node_features,\n","                config_features=config_features,\n","                graph=graph, num_configs=num_configs, edgeset_prefix='sampled_')\n","\n","            is_selected = graph.node_sets['op']['selected']\n","            # Need to expand twice as `is_selected` is a vector (num_nodes) but\n","            # x_{backprop, full} are 3D tensors (num_nodes, num_configs, num_feats).\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            x = tf.where(is_selected, x_backprop, x_full)\n","        else:\n","            x = x_full\n","\n","        adj_config = AdjacencyMultiplier(graph, 'config')\n","\n","        # Features for configurable nodes.\n","        config_feats = (adj_config.transpose() @ x)\n","\n","        # Global pooling\n","        adj_pool_op_sum = AdjacencyMultiplier(graph, 'g_op').transpose()\n","        adj_pool_op_mean = adj_pool_op_sum.normalize_right()\n","        adj_pool_config_sum = AdjacencyMultiplier(\n","            graph, 'g_config').transpose()\n","        x = self._postnet(tf.concat([\n","            # (A D^-1) @ Features\n","            adj_pool_op_mean @ x,\n","            # l2_normalize( A @ Features )\n","            tf.nn.l2_normalize(adj_pool_op_sum @ x, axis=-1),\n","            # l2_normalize( A @ Features )\n","            tf.nn.l2_normalize(adj_pool_config_sum @ config_feats, axis=-1),\n","        ], axis=-1))\n","\n","        x = tf.squeeze(x, -1)\n","\n","        return x\n","\n","def _mlp(dims, hidden_activation, l2reg=1e-4, use_bias=True):\n","  \"\"\"Helper function for multi-layer perceptron (MLP).\"\"\"\n","  layers = []\n","  for i, dim in enumerate(dims):\n","    if i > 0:\n","      layers.append(tf.keras.layers.Activation(hidden_activation))\n","    layers.append(tf.keras.layers.Dense(\n","        dim, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n","        use_bias=use_bias))\n","  return tf.keras.Sequential(layers)\n","\n","\"\"\"\n","  Layout Training Pipeline\n","  PARAMETERS\n","  * Batch Size information\n","    - BATCH_SIZE: number of graphs per batch\n","    - CONFIGS_PER_GRAPH: number of configurations (features and target values) per graph\n","    - MAX_KEEP_NODES: useful for dropout\n","  * Collection to train on\n","    - SOURCE: can be 'xla' or 'nlp'\n","    - SEARCH: can be 'random' or 'default'\n","  \"\"\"\n","\n","\"\"\"\n","CREATE DATASETS FOR TRAINING\n","\"\"\"\n","\n","def pull_data(CONFIGS_PER_GRAPH, MAX_TRAIN_CONFIGS, MAX_NUM_CONFIGS, MAX_KEEP_NODES, BATCH_SIZE, layout_data_root_dir):\n","  layout_npz_dataset = get_npz_dataset(\n","      layout_data_root_dir,\n","      min_train_configs=CONFIGS_PER_GRAPH,\n","      max_train_configs= MAX_NUM_CONFIGS,  # If any graph has more than this configurations, it will be filtered [speeds up loading + training]\n","      cache_dir='cache'\n","  )\n","\n","  layout_train_ds = (\n","        layout_npz_dataset.train.get_graph_tensors_dataset(\n","            config_samples = CONFIGS_PER_GRAPH,\n","            max_nodes=MAX_KEEP_NODES)\n","        .shuffle(100, reshuffle_each_iteration=True)\n","        .batch(BATCH_SIZE, drop_remainder=False)\n","        .map(tfgnn.GraphTensor.merge_batch_to_components)\n","        .map(pair_layout_graph_with_label))\n","\n","  layout_valid_ds = (\n","        layout_npz_dataset.validation.get_graph_tensors_dataset(\n","            config_samples = CONFIGS_PER_GRAPH)\n","        .batch(BATCH_SIZE, drop_remainder=False)\n","        .map(tfgnn.GraphTensor.merge_batch_to_components)\n","        .map(pair_layout_graph_with_label))\n","\n","  return layout_npz_dataset, layout_train_ds, layout_valid_ds\n","\n","\n","\n","def create_model(CONFIGS_PER_GRAPH, layout_npz_dataset):\n","  model = ResModel(CONFIGS_PER_GRAPH, layout_npz_dataset.num_ops)\n","\n","  loss = tfr.keras.losses.ListMLELoss()  # (temperature=10)\n","  opt = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=0.5)\n","\n","  model.compile(loss=loss, optimizer=opt, metrics=[\n","      tfr.keras.metrics.OPAMetric(name='opa_metric'),\n","  ])\n","\n","  return model\n","\n","def train_model(model, epochs, layout_train_ds, layout_valid_ds):\n","  best_val_opa = -1  # Tracks best validation OPA\n","  best_val_at_epoch = -1  # At which epoch.\n","\n","  for i in range(epochs):\n","      history = model.fit(\n","          layout_train_ds, epochs=1, verbose=1, validation_data=layout_valid_ds,\n","          validation_freq=1)\n","\n","      train_loss = history.history['loss'][-1]\n","      train_opa = history.history['opa_metric'][-1]\n","      val_loss = history.history['val_loss'][-1]\n","      val_opa = history.history['val_opa_metric'][-1]\n","      if val_opa > best_val_opa:\n","          best_val_opa = val_opa\n","          best_val_at_epoch = i\n","          best_params = {v.ref: v + 0 for v in model.trainable_variables}\n","          print(' * [@%i] Validation (NEW BEST): %s' % (i, str(val_opa)))\n","      elif early_stop > 0 and i - best_val_at_epoch >= early_stop:\n","        print('[@%i] Best accuracy was attained at epoch %i. Stopping.' % (i, best_val_at_epoch))\n","        break\n","  # Restore best parameters.\n","  print('Restoring parameters corresponding to the best validation OPA.')\n","  assert best_params is not None\n","  for v in model.trainable_variables:\n","      v.assign(best_params[v.ref])\n","\n","  return model, train_loss, train_opa, val_loss, val_opa, best_params\n","\n","\n","def run_inference(model, _INFERENCE_CONFIGS_BATCH_SIZE, layout_npz_dataset):\n","  print('\\n\\n   Running inference on test set ...\\n\\n')\n","  test_rankings = []\n","\n","  assert layout_npz_dataset.test.graph_id is not None\n","  for graph in tqdm.tqdm(layout_npz_dataset.test.iter_graph_tensors(),\n","                        total=layout_npz_dataset.test.graph_id.shape[-1],\n","                        desc='Inference'):\n","      # print(graph)\n","      num_configs = graph.node_sets['g']['runtimes'].shape[-1]\n","      # print(num_configs)\n","      # print(MAX_KEEP_NODES)\n","      # print(\"\\n\\n\\n\")\n","      all_scores = []\n","      for i in tqdm.tqdm(range(0, num_configs, _INFERENCE_CONFIGS_BATCH_SIZE)):\n","          end_i = min(i + _INFERENCE_CONFIGS_BATCH_SIZE, num_configs)\n","          # Take a cut of the configs.\n","          node_set_g = graph.node_sets['g']\n","          subconfigs_graph = tfgnn.GraphTensor.from_pieces(\n","              edge_sets=graph.edge_sets,\n","              node_sets={\n","                  'op': graph.node_sets['op'],\n","                  'nconfig': tfgnn.NodeSet.from_fields(\n","                      sizes=graph.node_sets['nconfig'].sizes,\n","                      features={\n","                          'feats': graph.node_sets['nconfig']['feats'][:, i:end_i],\n","                      }),\n","                  'g': tfgnn.NodeSet.from_fields(\n","                      sizes=tf.constant([1]),\n","                      features={\n","                          'graph_id': node_set_g['graph_id'],\n","                          'runtimes': node_set_g['runtimes'][:, i:end_i],\n","                          'kept_node_ratio': node_set_g['kept_node_ratio'],\n","                      })\n","              })\n","          h = model.forward(subconfigs_graph, num_configs=(end_i - i),\n","                            backprop=False)\n","          all_scores.append(h[0])\n","      all_scores = tf.concat(all_scores, axis=0)\n","      graph_id = graph.node_sets['g']['graph_id'][0].numpy().decode()\n","      sorted_indices = tf.strings.join(\n","          tf.strings.as_string(tf.argsort(all_scores)), ';').numpy().decode()\n","      test_rankings.append((graph_id, sorted_indices))\n","  return test_rankings\n","\n","def write_output(test_rankings, output_csv_filename, SOURCE, SEARCH):\n","    with tf.io.gfile.GFile(output_csv_filename, 'w') as fout:\n","        fout.write('ID,TopConfigs\\n')\n","        for graph_id, ranks in test_rankings:\n","            fout.write(f'layout:{SOURCE}:{SEARCH}:{graph_id},{ranks}\\n')\n","    print('\\n\\n   ***  Wrote', output_csv_filename, '\\n\\n')\n","\n","\"\"\"\n","BEGIN RUNNING CODE!!!\n","THERE ARE SETTINGS AND HYPERPARAMETERES\n","\"\"\"\n","\n","def main(source, search, **kwargs):\n","  print(\"AAA\")\n","  # need to download npz\n","  # LAYOUT_DATA_ROOT = '/npz/layout'\n","  LAYOUT_DATA_ROOT = '/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout'\n","  SOURCE = source  # Can be \"xla\" or \"nlp\"\n","  SEARCH = search  # Can be \"random\" or \"default\"\n","\n","  layout_data_root_dir = os.path.join(\n","        os.path.expanduser(LAYOUT_DATA_ROOT), SOURCE, SEARCH)\n","\n","  # Batch size information.\n","  # BATCH_SIZE = 10  # Number of graphs per batch.\n","  # CONFIGS_PER_GRAPH = 2  # Number of configurations (features and target values) per graph.\n","  # MAX_NUM_CONFIGS = 20 # maximum number of configurations to filter for\n","  # MAX_KEEP_NODES = 100  # Useful for dropout.\n","  # MAX_TRAIN_CONFIGS = 20\n","\n","  BATCH_SIZE = kwargs['batch_size']  # Number of graphs per batch.\n","  CONFIGS_PER_GRAPH = kwargs['configs_per_graph']  # Number of configurations (features and target values) per graph.\n","  MAX_NUM_CONFIGS = kwargs['max_num_configs'] # maximum number of configurations to filter for\n","  MAX_KEEP_NODES = kwargs['max_keep_nodes']  # Useful for dropout.\n","  MAX_TRAIN_CONFIGS = kwargs['max_train_configs']\n","\n","  # edges \"sampled_config\" and \"sampled_feed\" (or, \"con50fig\" and \"feed\")\n","  early_stop = 5  # If validation OPA did not increase in this many epochs, terminate training.\n","  best_params = None  # Stores parameters corresponding to best validation OPA, to restore to them after training.\n","  epochs = 1  # Total number of training epochs.\n","\n","  # pull the data\n","  layout_npz_dataset, layout_train_ds, layout_valid_ds = pull_data(CONFIGS_PER_GRAPH, MAX_TRAIN_CONFIGS, MAX_NUM_CONFIGS, MAX_KEEP_NODES, BATCH_SIZE, layout_data_root_dir)\n","  model = create_model(CONFIGS_PER_GRAPH, layout_npz_dataset)\n","  model, train_loss, train_opa, val_loss, val_opa, best_params = train_model(model, epochs, layout_train_ds, layout_valid_ds)\n","\n","\n","  _INFERENCE_CONFIGS_BATCH_SIZE = 50\n","  # _INFERENCE_CONFIGS_BATCH_SIZE = 100\n","\n","  folder_path = '/content/drive/MyDrive/tpu_graphs/outputcsvs/'\n","  output_csv_filename = f'inference_layout_{SOURCE}_{SEARCH}.csv'\n","  output_csv_filename = folder_path + output_csv_filename\n","\n","  test_rankings = run_inference(model, _INFERENCE_CONFIGS_BATCH_SIZE, layout_npz_dataset)\n","  write_output(test_rankings, output_csv_filename, SOURCE, SEARCH)"]},{"cell_type":"markdown","metadata":{"id":"0Lz-bLTSa8HH"},"source":["# Params to Configure\n","\n","For every Kaggle submission, the same hyperparameters need to be used four times, in order to produce 4 output CSVs. The 4 configurations are:\n","1. source = nlp; search = random\n","2. source = nlp; search = default\n","3. source = xla; search = random\n","4. source = xla; search = default\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.602156Z","iopub.status.busy":"2023-11-16T23:52:27.601880Z","iopub.status.idle":"2023-11-16T23:52:27.607044Z","shell.execute_reply":"2023-11-16T23:52:27.606025Z","shell.execute_reply.started":"2023-11-16T23:52:27.602132Z"},"id":"DDbaXawQNUNZ","trusted":true},"outputs":[],"source":["source = 'nlp' # Can be \"xla\" or \"nlp\"\n","search = 'random' # Can be \"random\" or \"default\"\n","\n","batch_size = 10  # Number of graphs per batch.\n","configs_per_graph = 2  # Number of configurations (features and target values) per graph.\n","max_num_configs = 20 # maximum number of configurations to filter for\n","max_keep_nodes = 100  # Useful for dropout.\n","max_train_configs = 20 # If any graph has more than this configurations, it will be filtered [speeds up loading + training]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.608503Z","iopub.status.busy":"2023-11-16T23:52:27.608176Z"},"executionInfo":{"elapsed":1181867,"status":"ok","timestamp":1700110797067,"user":{"displayName":"Valeria Vera Lagos","userId":"03700786808723630376"},"user_tz":300},"id":"0QOrsI3L4IEX","outputId":"031e610b-8ed6-4b2d-9bba-a28d2e5a7052","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AAA\n","dataset cache file:  cache/da030f462f243e051c33e7eb1886d667-cache.npz\n","loaded from cache/da030f462f243e051c33e7eb1886d667-cache.npz\n","dataset cache file:  cache/4378df730c04529a89e472ddf67e9460-cache.npz\n","loaded from cache/4378df730c04529a89e472ddf67e9460-cache.npz\n","dataset cache file:  cache/66a2b3a0bac484e0aec26d531f74b257-cache.npz\n","loaded from cache/66a2b3a0bac484e0aec26d531f74b257-cache.npz\n","21/21 [==============================] - 205s 9s/step - loss: 0.7109 - opa_metric: 0.5556 - val_loss: 0.6971 - val_opa_metric: 0.6000\n"," * [@0] Validation (NEW BEST): 0.6000000238418579\n","Restoring parameters corresponding to the best validation OPA.\n","\n","\n","   Running inference on test set ...\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Inference:   0%|          | 0/17 [00:00<?, ?it/s]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:01<00:36,  1.90s/it]\u001b[A\n"," 10%|         | 2/20 [00:03<00:32,  1.82s/it]\u001b[A\n"," 15%|        | 3/20 [00:05<00:30,  1.80s/it]\u001b[A\n"," 20%|        | 4/20 [00:07<00:28,  1.79s/it]\u001b[A\n"," 25%|       | 5/20 [00:09<00:26,  1.79s/it]\u001b[A\n"," 30%|       | 6/20 [00:10<00:25,  1.79s/it]\u001b[A\n"," 35%|      | 7/20 [00:12<00:23,  1.78s/it]\u001b[A\n"," 40%|      | 8/20 [00:14<00:21,  1.78s/it]\u001b[A\n"," 45%|     | 9/20 [00:16<00:19,  1.77s/it]\u001b[A\n"," 50%|     | 10/20 [00:17<00:17,  1.78s/it]\u001b[A\n"," 55%|    | 11/20 [00:19<00:15,  1.77s/it]\u001b[A\n"," 60%|    | 12/20 [00:21<00:14,  1.77s/it]\u001b[A\n"," 65%|   | 13/20 [00:23<00:12,  1.77s/it]\u001b[A\n"," 70%|   | 14/20 [00:24<00:10,  1.77s/it]\u001b[A\n"," 75%|  | 15/20 [00:26<00:08,  1.78s/it]\u001b[A\n"," 80%|  | 16/20 [00:28<00:07,  1.78s/it]\u001b[A\n"," 85%| | 17/20 [00:30<00:05,  1.77s/it]\u001b[A\n"," 90%| | 18/20 [00:32<00:03,  1.77s/it]\u001b[A\n"," 95%|| 19/20 [00:33<00:01,  1.77s/it]\u001b[A\n","100%|| 20/20 [00:35<00:00,  1.78s/it]\u001b[A\n","Inference:   6%|         | 1/17 [00:36<09:41, 36.33s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:06<02:03,  6.48s/it]\u001b[A\n"," 10%|         | 2/20 [00:12<01:56,  6.45s/it]\u001b[A\n"," 15%|        | 3/20 [00:19<01:49,  6.41s/it]\u001b[A\n"," 20%|        | 4/20 [00:25<01:42,  6.40s/it]\u001b[A\n"," 25%|       | 5/20 [00:32<01:36,  6.41s/it]\u001b[A\n"," 30%|       | 6/20 [00:38<01:29,  6.43s/it]\u001b[A\n"," 35%|      | 7/20 [00:44<01:23,  6.42s/it]\u001b[A\n"," 40%|      | 8/20 [00:51<01:17,  6.42s/it]\u001b[A\n"," 45%|     | 9/20 [00:57<01:10,  6.40s/it]\u001b[A\n"," 50%|     | 10/20 [01:04<01:04,  6.41s/it]\u001b[A\n"," 55%|    | 11/20 [01:10<00:57,  6.42s/it]\u001b[A\n"," 60%|    | 12/20 [01:16<00:51,  6.40s/it]\u001b[A\n"," 65%|   | 13/20 [01:23<00:44,  6.41s/it]\u001b[A\n"," 70%|   | 14/20 [01:29<00:38,  6.42s/it]\u001b[A\n"," 75%|  | 15/20 [01:36<00:32,  6.45s/it]\u001b[A\n"," 80%|  | 16/20 [01:42<00:25,  6.45s/it]\u001b[A\n"," 85%| | 17/20 [01:49<00:19,  6.43s/it]\u001b[A\n"," 90%| | 18/20 [01:55<00:12,  6.44s/it]\u001b[A\n"," 95%|| 19/20 [02:02<00:06,  6.44s/it]\u001b[A\n","100%|| 20/20 [02:08<00:00,  6.43s/it]\u001b[A\n","Inference:  12%|        | 2/17 [02:45<22:43, 90.92s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:04<01:18,  4.13s/it]\u001b[A\n"," 10%|         | 2/20 [00:08<01:13,  4.06s/it]\u001b[A\n"," 15%|        | 3/20 [00:12<01:08,  4.05s/it]\u001b[A\n"," 20%|        | 4/20 [00:16<01:04,  4.04s/it]\u001b[A\n"," 25%|       | 5/20 [00:20<01:00,  4.03s/it]\u001b[A\n"," 30%|       | 6/20 [00:24<00:56,  4.04s/it]\u001b[A\n"," 35%|      | 7/20 [00:28<00:52,  4.06s/it]\u001b[A\n"," 40%|      | 8/20 [00:32<00:49,  4.09s/it]\u001b[A\n"," 45%|     | 9/20 [00:36<00:44,  4.09s/it]\u001b[A\n"," 50%|     | 10/20 [00:40<00:40,  4.09s/it]\u001b[A\n"," 55%|    | 11/20 [00:44<00:36,  4.07s/it]\u001b[A\n"," 60%|    | 12/20 [00:48<00:32,  4.08s/it]\u001b[A\n"," 65%|   | 13/20 [00:53<00:28,  4.12s/it]\u001b[A\n"," 70%|   | 14/20 [00:57<00:24,  4.11s/it]\u001b[A\n"," 75%|  | 15/20 [01:01<00:20,  4.10s/it]\u001b[A\n"," 80%|  | 16/20 [01:05<00:16,  4.08s/it]\u001b[A\n"," 85%| | 17/20 [01:09<00:12,  4.07s/it]\u001b[A\n"," 90%| | 18/20 [01:13<00:08,  4.06s/it]\u001b[A\n"," 95%|| 19/20 [01:17<00:04,  4.05s/it]\u001b[A\n","100%|| 20/20 [01:21<00:00,  4.07s/it]\u001b[A\n","Inference:  18%|        | 3/17 [04:07<20:16, 86.87s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:14<04:41, 14.82s/it]\u001b[A\n"," 10%|         | 2/20 [00:29<04:25, 14.78s/it]\u001b[A\n"," 15%|        | 3/20 [00:44<04:11, 14.81s/it]\u001b[A\n"," 20%|        | 4/20 [00:59<03:56, 14.79s/it]\u001b[A\n"," 25%|       | 5/20 [01:14<03:42, 14.81s/it]\u001b[A\n"," 30%|       | 6/20 [01:28<03:26, 14.78s/it]\u001b[A\n"," 35%|      | 7/20 [01:43<03:12, 14.79s/it]\u001b[A\n"," 40%|      | 8/20 [01:58<02:57, 14.77s/it]\u001b[A\n"," 45%|     | 9/20 [02:13<02:42, 14.79s/it]\u001b[A\n"," 50%|     | 10/20 [02:27<02:27, 14.76s/it]\u001b[A\n"," 55%|    | 11/20 [02:42<02:13, 14.78s/it]\u001b[A\n"," 60%|    | 12/20 [02:57<01:58, 14.76s/it]\u001b[A\n"," 65%|   | 13/20 [03:12<01:43, 14.75s/it]\u001b[A\n"," 70%|   | 14/20 [03:26<01:28, 14.77s/it]\u001b[A\n"," 75%|  | 15/20 [03:41<01:13, 14.78s/it]\u001b[A\n"," 80%|  | 16/20 [03:56<00:59, 14.77s/it]\u001b[A\n"," 85%| | 17/20 [04:11<00:44, 14.78s/it]\u001b[A\n"," 90%| | 18/20 [04:26<00:29, 14.78s/it]\u001b[A\n"," 95%|| 19/20 [04:40<00:14, 14.78s/it]\u001b[A\n","100%|| 20/20 [04:55<00:00, 14.78s/it]\u001b[A\n","Inference:  24%|       | 4/17 [09:03<36:43, 169.49s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:02<00:43,  2.29s/it]\u001b[A\n"," 10%|         | 2/20 [00:04<00:41,  2.30s/it]\u001b[A\n"," 15%|        | 3/20 [00:06<00:39,  2.30s/it]\u001b[A\n"," 20%|        | 4/20 [00:09<00:36,  2.29s/it]\u001b[A\n"," 25%|       | 5/20 [00:11<00:34,  2.28s/it]\u001b[A\n"," 30%|       | 6/20 [00:13<00:32,  2.30s/it]\u001b[A\n"," 35%|      | 7/20 [00:16<00:29,  2.29s/it]\u001b[A\n"," 40%|      | 8/20 [00:18<00:27,  2.28s/it]\u001b[A\n"," 45%|     | 9/20 [00:20<00:25,  2.28s/it]\u001b[A\n"," 50%|     | 10/20 [00:22<00:22,  2.29s/it]\u001b[A\n"," 55%|    | 11/20 [00:25<00:20,  2.28s/it]\u001b[A\n"," 60%|    | 12/20 [00:27<00:18,  2.28s/it]\u001b[A\n"," 65%|   | 13/20 [00:29<00:15,  2.27s/it]\u001b[A\n"," 70%|   | 14/20 [00:31<00:13,  2.27s/it]\u001b[A\n"," 75%|  | 15/20 [00:34<00:11,  2.28s/it]\u001b[A\n"," 80%|  | 16/20 [00:36<00:09,  2.27s/it]\u001b[A\n"," 85%| | 17/20 [00:38<00:06,  2.27s/it]\u001b[A\n"," 90%| | 18/20 [00:41<00:04,  2.27s/it]\u001b[A\n"," 95%|| 19/20 [00:43<00:02,  2.28s/it]\u001b[A\n","100%|| 20/20 [00:45<00:00,  2.28s/it]\u001b[A\n","Inference:  29%|       | 5/17 [09:49<25:00, 125.04s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|         | 1/20 [00:01<00:35,  1.88s/it]\u001b[A\n"," 10%|         | 2/20 [00:03<00:33,  1.89s/it]\u001b[A\n"," 15%|        | 3/20 [00:05<00:32,  1.91s/it]\u001b[A\n"," 20%|        | 4/20 [00:07<00:30,  1.91s/it]\u001b[A\n"," 25%|       | 5/20 [00:09<00:28,  1.90s/it]\u001b[A\n"," 30%|       | 6/20 [00:11<00:26,  1.90s/it]\u001b[A\n"," 35%|      | 7/20 [00:13<00:24,  1.90s/it]\u001b[A\n"," 40%|      | 8/20 [00:15<00:22,  1.90s/it]\u001b[A\n"," 45%|     | 9/20 [00:17<00:20,  1.91s/it]\u001b[A\n"," 50%|     | 10/20 [00:19<00:19,  1.91s/it]\u001b[A\n"," 55%|    | 11/20 [00:20<00:17,  1.91s/it]\u001b[A\n"," 60%|    | 12/20 [00:22<00:15,  1.91s/it]\u001b[A\n"," 65%|   | 13/20 [00:24<00:13,  1.90s/it]\u001b[A\n"," 70%|   | 14/20 [00:26<00:11,  1.91s/it]\u001b[A\n"," 75%|  | 15/20 [00:28<00:09,  1.92s/it]\u001b[A\n"," 80%|  | 16/20 [00:30<00:07,  1.91s/it]\u001b[A\n"," 85%| | 17/20 [00:32<00:05,  1.91s/it]\u001b[A\n"," 90%| | 18/20 [00:34<00:03,  1.90s/it]\u001b[A"]}],"source":["main(source = 'nlp',\n","     search = 'random',\n","      batch_size = batch_size,\n","     configs_per_graph = configs_per_graph,\n","     max_num_configs = max_num_configs,\n","     max_keep_nodes = max_keep_nodes,\n","     max_train_configs = max_train_configs\n","     )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6Ag2Dlv4mxV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6641124,"sourceId":58266,"sourceType":"competition"},{"datasetId":4014081,"sourceId":6984420,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
