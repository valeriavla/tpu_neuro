{"cells":[{"cell_type":"markdown","metadata":{"id":"7HUOn3rWazmi"},"source":["# Instructions\n","\n","Make a copy of this notebook, and save with your initials at the end, so that we do not overwrite each other's.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:00.066754Z","iopub.status.busy":"2023-11-16T23:52:00.066374Z","iopub.status.idle":"2023-11-16T23:52:23.775102Z","shell.execute_reply":"2023-11-16T23:52:23.773864Z","shell.execute_reply.started":"2023-11-16T23:52:00.066718Z"},"executionInfo":{"elapsed":98462,"status":"ok","timestamp":1700106283170,"user":{"displayName":"Valeria Vera Lagos","userId":"03700786808723630376"},"user_tz":300},"id":"kBlBXPqUg8Ti","outputId":"13dcc5a5-788c-4b3f-f108-ec9fcb81a822","trusted":true},"outputs":[],"source":["!pip install tensorflow_gnn --pre\n","!pip install tensorflow_ranking"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:23.777985Z","iopub.status.busy":"2023-11-16T23:52:23.777652Z","iopub.status.idle":"2023-11-16T23:52:27.401459Z","shell.execute_reply":"2023-11-16T23:52:27.400520Z","shell.execute_reply.started":"2023-11-16T23:52:23.777948Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","import tensorflow_ranking as tfr\n","import argparse\n","import tqdm\n","import collections\n","import functools\n","import hashlib\n","import io\n","import os\n","from typing import NamedTuple\n","from absl import flags"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### TILES\n","class FakeBoolFlag(NamedTuple):\n","    value: bool\n","_TOY_DATA = FakeBoolFlag(value=False)\n","\n","\n","class TileExample(NamedTuple):\n","  \"\"\"Single example of tile graph.\"\"\"\n","  node_features: tf.Tensor\n","  node_ops: tf.Tensor\n","  edges: tf.Tensor\n","  config_features: tf.Tensor\n","  config_runtimes: tf.Tensor\n","  config_runtime_normalizers: tf.Tensor\n","  tile_id: tf.Tensor\n","  total_nodes: tf.Tensor\n","  total_edges: tf.Tensor\n","  total_configs: tf.Tensor\n","\n","  def to_graph_tensor(\n","      self, config_samples: int = -1,\n","      normalize_runtimes: bool = True) -> tfgnn.GraphTensor:\n","    \"\"\"Packages instance tensors (edges, features) into `GraphTensor`.\n","\n","    Args:\n","      config_samples: if -1, then all module configurations (and their runtimes)\n","        are returned. If >=0, then this many module configurations (and their\n","        corresponding runtimes) are sampled uniformly at random.\n","      normalize_runtimes: If set (default), runtimes will be normalized by\n","        dividing over the runtime of \"default tile size\" (to account for worker\n","        machine differences).\n","\n","    Returns:\n","      GraphTensor with node-sets:\n","        + op (feats='op': int-categorical, 'feats': float-vector).\n","          This is the only \"real\" graph node\"\n","        + configs (feats='feats': float-vector, 'runtimes': float scalar,\n","                   'normalizers': float scalar).\n","          These are \"fake\" nodes. There will be one node per configuration.\n","        + g (stands for \"graph\") has one (root) node connecting to all op and\n","          config nodes.\n","      and edge-sets:\n","        + 'feed': directed edges connecting op-node to op-node.\n","        + 'g_op': edges connecting the singleton \"g\" node to every \"op\" node.\n","        + 'g_config': connecting the singleton \"g\" node to every \"config\" node.\n","    \"\"\"\n","    config_features = self.config_features\n","    config_runtimes = self.config_runtimes\n","    config_runtime_normalizers = self.config_runtime_normalizers\n","    num_configs = tf.shape(config_features)[0]\n","\n","    # If sampling is requested.\n","    if config_samples >= 0:\n","      rnd = tf.random.shuffle(tf.range(num_configs, dtype=tf.int32))\n","      rnd = rnd[:config_samples]\n","      config_features = tf.gather(config_features, rnd)\n","      config_runtimes = tf.gather(config_runtimes, rnd)\n","      config_runtime_normalizers = tf.gather(config_runtime_normalizers, rnd)\n","      num_configs = tf.shape(config_features)[0]\n","\n","    if normalize_runtimes:\n","      config_runtimes /= config_runtime_normalizers\n","\n","    return tfgnn.GraphTensor.from_pieces(\n","        node_sets={\n","            'op': tfgnn.NodeSet.from_fields(\n","                sizes=tf.expand_dims(self.total_nodes, 0),\n","                features={\n","                    'op': self.node_ops,\n","                    'feats': self.node_features,\n","                }\n","            ),\n","            'config': tfgnn.NodeSet.from_fields(\n","                features={\n","                    'feats': config_features,\n","                    'runtimes': config_runtimes,\n","                    'normalizers': config_runtime_normalizers,\n","                },\n","                sizes=tf.expand_dims(num_configs, 0),\n","            ),\n","            'g': tfgnn.NodeSet.from_fields(\n","                features={'tile_id': tf.expand_dims(self.tile_id, 0)},\n","                sizes=tf.constant([1]))\n","        },\n","        edge_sets={\n","            'feed': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.expand_dims(self.total_edges, 0),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('op', self.edges[:, 0]),\n","                    target=('op', self.edges[:, 1]))),\n","            'g_op': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.expand_dims(self.total_nodes, 0),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('g', tf.zeros([self.total_nodes], dtype=tf.int32)),\n","                    target=('op', tf.range(self.total_nodes, dtype=tf.int32)))),\n","            'g_config': tfgnn.EdgeSet.from_fields(\n","                sizes=tf.expand_dims(num_configs, 0),\n","                adjacency=tfgnn.Adjacency.from_indices(\n","                    source=('g', tf.zeros([num_configs], dtype=tf.int32)),\n","                    target=('config', tf.range(num_configs, dtype=tf.int32)))),\n","        })\n","\n","\n","class NpzDatasetPartition:\n","  \"\"\"Holds one data partition (train, test, validation) on device memory.\"\"\"\n","\n","  def __init__(self):\n","    # Populated in `add_npz_file()`.\n","    self._data_dict: dict[str, list[np.ndarray]] = collections.defaultdict(list)\n","    self._num_edges: list[int] = [0]    # prepend with 0 to prep for cumsum.\n","    self._num_configs: list[int] = [0]  # ^^\n","    self._num_nodes: list[int] = [0]    # ^^\n","\n","    # Populated in `finalize()`.\n","    self.node_feat: 'tf.Tensor | None' = None   # indexed by node_ranges.\n","    self.node_opcode: 'tf.Tensor | None' = None  # ^^\n","    self.edge_index: 'tf.Tensor | None' = None   # indexed by edge_ranges.\n","    self.config_feat: 'tf.Tensor | None' = None      # indexed by config_ranges.\n","    self.config_runtime: 'tf.Tensor | None' = None   # ^^\n","    self.config_runtime_normalizers: 'tf.Tensor | None' = None  # ^^\n","    self.tile_id: 'tf.Tensor | None' = None\n","\n","    # finalize() sets to: cumsum([0, numEdges(graph_1), numEdges(graph_2), ..]).\n","    self.edge_ranges: 'tf.Tensor | None' = None\n","    # finalize() sets to: cumsum([0, numNodes(graph_1), numNodes(graph_2), ..]).\n","    self.node_ranges: 'tf.Tensor | None' = None\n","    # finalize() sets to: cumsum([0, numModules(graph_1), nModul(graph_2), ..]).\n","    self.config_ranges: 'tf.Tensor | None' = None\n","\n","  def save_to_file(self, cache_file: str):\n","    \"\"\"Saves dataset as numpy. Can be restored with `load_from_file`.\"\"\"\n","    assert self.node_feat is not None, 'finalize() was not invoked'\n","    assert self.node_opcode is not None\n","    assert self.edge_index is not None\n","    assert self.config_feat is not None\n","    assert self.config_runtime is not None\n","    assert self.config_runtime_normalizers is not None\n","    assert self.tile_id is not None\n","    assert self.edge_ranges is not None\n","    assert self.node_ranges is not None\n","    assert self.config_ranges is not None\n","    np_dict = dict(\n","        node_feat=self.node_feat.numpy(),\n","        node_opcode=self.node_opcode.numpy(),\n","        edge_index=self.edge_index.numpy(),\n","        config_feat=self.config_feat.numpy(),\n","        config_runtime=self.config_runtime.numpy(),\n","        config_runtime_normalizers=self.config_runtime_normalizers.numpy(),\n","        edge_ranges=self.edge_ranges.numpy(),\n","        node_ranges=self.node_ranges.numpy(),\n","        config_ranges=self.config_ranges.numpy()\n","    )\n","    bytes_io = io.BytesIO()\n","    np.savez_compressed(bytes_io, **np_dict)\n","    with tf.io.gfile.GFile(cache_file, 'wb') as fout:\n","      fout.write(bytes_io.getvalue())\n","    print('wrote ' + cache_file)\n","    tile_ids_file = cache_file + '.tiles.txt'\n","    with tf.io.gfile.GFile(tile_ids_file, 'w') as fout:\n","      fout.write(b'\\n'.join(self.tile_id.numpy().tolist()).decode())\n","    print('wrote ' + tile_ids_file)\n","\n","  def load_from_file(self, cache_file: str):\n","    \"\"\"Loads dataset from numpy file.\"\"\"\n","    np_dict = np.load(tf.io.gfile.GFile(cache_file, 'rb'))\n","    self.node_feat = tf.constant(np_dict['node_feat'])\n","    self.node_opcode = tf.constant(np_dict['node_opcode'])\n","    self.edge_index = tf.constant(np_dict['edge_index'])\n","    self.config_feat = tf.constant(np_dict['config_feat'])\n","    self.config_runtime = tf.constant(np_dict['config_runtime'])\n","    self.config_runtime_normalizers = tf.constant(\n","        np_dict['config_runtime_normalizers'])\n","    self.edge_ranges = tf.constant(np_dict['edge_ranges'])\n","    self.node_ranges = tf.constant(np_dict['node_ranges'])\n","    self.config_ranges = tf.constant(np_dict['config_ranges'])\n","    tile_ids = tf.io.gfile.GFile(cache_file + '.tiles.txt', 'r').readlines()\n","    self.tile_id = tf.stack([tile_id.rstrip() for tile_id in tile_ids])\n","    print('loaded from ' + cache_file)\n","\n","  def add_npz_file(\n","      self, tile_id: str, npz_file: np.lib.npyio.NpzFile, min_configs: int = 2):\n","    \"\"\"Copies data from npz file into this class instance.\n","\n","    After finishing all calls `add_npz_file()`, user must invoke `finalize()`.\n","\n","    Args:\n","      tile_id: the filename (without extension) that npz_file was read from.\n","      npz_file: Output of np.load on a file from the TpuGraphs Tiles dataset.\n","      min_configs: The file be incorporated only if the number of module\n","        configurations is equal or greater than this.\n","    \"\"\"\n","    npz_data = dict(npz_file.items())\n","    #num_configs = npz_data['config_feat'].shape[0]\n","    num_configs = npz_data['node_config_feat'].shape[0]\n","    if num_configs < min_configs:\n","      print('skipping tile with only %i configurations' % num_configs)\n","      return\n","    for key, ndarray in npz_data.items():\n","      self._data_dict[key].append(ndarray)\n","    self._data_dict['tile_id'].append(np.array(tile_id))\n","    num_nodes = npz_data['node_feat'].shape[0]\n","    num_edges = npz_data['edge_index'].shape[0]\n","    assert num_nodes == npz_data['node_opcode'].shape[0]\n","    assert num_configs == npz_data['config_runtime'].shape[0]\n","    #assert num_configs == npz_data['config_runtime_normalizers'].shape[0]\n","    assert num_configs == npz_data['config_runtime'].shape[0]\n","    self._num_nodes.append(num_nodes)\n","    self._num_edges.append(num_edges)\n","    self._num_configs.append(num_configs)\n","\n","  def finalize(self):\n","    self.tile_id = tf.stack(self._data_dict['tile_id'], axis=0)\n","    self.node_feat = tf.concat(self._data_dict['node_feat'], axis=0)\n","    self.node_opcode = tf.concat(self._data_dict['node_opcode'], axis=0)\n","    self.edge_index = tf.concat(self._data_dict['edge_index'], axis=0)\n","    self.config_feat = tf.concat(self._data_dict['config_feat'], axis=0)\n","    self.config_runtime = tf.concat(self._data_dict['config_runtime'], axis=0)\n","    self.config_runtime_normalizers = tf.concat(\n","        self._data_dict['config_runtime_normalizers'], axis=0)\n","    self.edge_ranges = tf.cumsum(self._num_edges)\n","    self.node_ranges = tf.cumsum(self._num_nodes)\n","    self.config_ranges = tf.cumsum(self._num_configs)\n","\n","  def get_item(self, index: int) -> TileExample:\n","    node_start = self.node_ranges[index]\n","    node_end = self.node_ranges[index + 1]\n","    edge_start = self.edge_ranges[index]\n","    edge_end = self.edge_ranges[index + 1]\n","    config_start = self.config_ranges[index]\n","    config_end = self.config_ranges[index + 1]\n","\n","    return TileExample(\n","        node_features=self.node_feat[node_start:node_end],\n","        node_ops=self.node_opcode[node_start:node_end],\n","        edges=self.edge_index[edge_start:edge_end],\n","        config_features=self.config_feat[config_start:config_end],\n","        config_runtimes=self.config_runtime[config_start:config_end],\n","        config_runtime_normalizers=(\n","            self.config_runtime_normalizers[config_start:config_end]),\n","        tile_id=self.tile_id[index],\n","        total_nodes=node_end - node_start,\n","        total_edges=edge_end - edge_start,\n","        total_configs=config_end - config_start)\n","\n","  def get_graph_tensors_dataset(\n","      self, config_samples: int = -1) -> tf.data.Dataset:\n","    if self.edge_ranges is None:\n","      raise ValueError('finalize() was not invoked.')\n","    dataset = tf.data.Dataset.range(self.edge_ranges.shape[0] - 1)\n","    dataset = dataset.map(self.get_item, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.map(\n","        functools.partial(TileExample.to_graph_tensor,\n","                          config_samples=config_samples))\n","    return dataset\n","\n","\n","def get_npz_split(\n","    split_path: str, min_configs=2, cache_dir=None) -> NpzDatasetPartition:\n","  \"\"\"Returns data for a single partition.\"\"\"\n","  glob_pattern = os.path.join(split_path, '*.npz')\n","  files = tf.io.gfile.glob(glob_pattern)\n","  if not files:\n","    raise ValueError('No files matched: ' + glob_pattern)\n","  if _TOY_DATA.value:\n","    files = files[:100]\n","\n","  cache_filename = None\n","  if cache_dir:\n","    if not tf.io.gfile.exists(cache_dir):\n","      tf.io.gfile.makedirs(cache_dir)\n","    filename_hash = hashlib.md5(\n","        f'{split_path}:{min_configs}:{_TOY_DATA.value}'.encode()).hexdigest()\n","    cache_filename = os.path.join(cache_dir, f'{filename_hash}-cache.npz')\n","    print('dataset cache file: ', cache_filename)\n","\n","  npz_dataset = NpzDatasetPartition()\n","  if cache_filename and tf.io.gfile.exists(cache_filename):\n","    npz_dataset.load_from_file(cache_filename)\n","  else:\n","    for filename in tqdm.tqdm(files):\n","      np_data = np.load(tf.io.gfile.GFile(filename, 'rb'))\n","      tile_id = os.path.splitext(os.path.basename(filename))[0]\n","      npz_dataset.add_npz_file(tile_id, np_data, min_configs=min_configs)\n","    npz_dataset.finalize()\n","    if cache_filename:\n","      npz_dataset.save_to_file(cache_filename)\n","\n","  return npz_dataset\n","\n","\n","class NpzDataset(NamedTuple):\n","  \"\"\"Contains all partitions of the dataset.\"\"\"\n","  train: NpzDatasetPartition\n","  validation: NpzDatasetPartition\n","  test: NpzDatasetPartition\n","\n","  @property\n","  def num_ops(self):\n","    return int(\n","        tf.reduce_max([\n","            tf.reduce_max(self.train.node_opcode),\n","            tf.reduce_max(self.validation.node_opcode),\n","            tf.reduce_max(self.test.node_opcode),\n","        ]).numpy()) + 1\n","\n","  def _get_normalizer(self, feature_matrix) -> tuple[\n","      tf.Tensor, tf.Tensor, tf.Tensor]:\n","    max_feat = tf.reduce_max(feature_matrix, axis=0, keepdims=True)\n","    min_feat = tf.reduce_min(feature_matrix, axis=0, keepdims=True)\n","    return min_feat[0] != max_feat[0], min_feat, max_feat\n","\n","  def _apply_normalizer(self, feature_matrix, used_columns, min_feat, max_feat):\n","    feature_matrix = tf.boolean_mask(feature_matrix, used_columns, axis=1)\n","    min_feat = tf.boolean_mask(min_feat, used_columns, axis=1)\n","    max_feat = tf.boolean_mask(max_feat, used_columns, axis=1)\n","    return (feature_matrix - min_feat) / (max_feat - min_feat)\n","\n","  def normalize(self):\n","    \"\"\"Removes constant features and normalizes remaining onto [0, 1].\n","\n","    The statistics are computed only from train partition then applied to all\n","    partitions {train, test, validation}.\n","    \"\"\"\n","    normalizer_args = self._get_normalizer(self.train.node_feat)\n","    self.train.node_feat = self._apply_normalizer(\n","        self.train.node_feat, *normalizer_args)\n","    self.validation.node_feat = self._apply_normalizer(\n","        self.validation.node_feat, *normalizer_args)\n","    self.test.node_feat = self._apply_normalizer(\n","        self.test.node_feat, *normalizer_args)\n","\n","    normalizer_args = self._get_normalizer(self.train.config_feat)\n","    self.train.config_feat = self._apply_normalizer(\n","        self.train.config_feat, *normalizer_args)\n","    self.validation.config_feat = self._apply_normalizer(\n","        self.validation.config_feat, *normalizer_args)\n","    self.test.config_feat = self._apply_normalizer(\n","        self.test.config_feat, *normalizer_args)\n","\n","\n","def get_npz_dataset(\n","    root_path: str, min_train_configs=-1,\n","    cache_dir: 'None | str' = None) -> NpzDataset:\n","  \"\"\"Returns {train, test, validation} partitions of tiles dataset collection.\n","\n","  All partitions will be normalized: statistics are computed from training set\n","  partition and applied to all partitions.\n","\n","  Args:\n","    root_path: Path where dataset lives. It must have subdirectories 'train',\n","      'test' and 'valid'.\n","    min_train_configs: If > 0, then tile examples will be filtered to have at\n","      least this many configurations (features and runtimes).\n","    cache_dir: If given, the many files for each of {train, test, validation}\n","      will be stored as one file (makes loading faster, for future runs).\n","  \"\"\"\n","  npz_dataset = NpzDataset(\n","      train=get_npz_split(\n","          os.path.join(root_path, 'train'), min_configs=min_train_configs,\n","          cache_dir=cache_dir),\n","      validation=get_npz_split(\n","          os.path.join(root_path, 'valid'), cache_dir=cache_dir),\n","      test=get_npz_split(\n","          os.path.join(root_path, 'test'), cache_dir=cache_dir))\n","  npz_dataset.normalize()\n","  return npz_dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.403245Z","iopub.status.busy":"2023-11-16T23:52:27.402763Z","iopub.status.idle":"2023-11-16T23:52:27.443507Z","shell.execute_reply":"2023-11-16T23:52:27.442701Z","shell.execute_reply.started":"2023-11-16T23:52:27.403218Z"},"trusted":true},"outputs":[],"source":["###IMPLICIT\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","\n","EPSILON = 1e-6  # To prevent division by 0.\n","\n","\n","class Multiplier:\n","  \"\"\"Holds an (implicit) matrix that can be multiplied with dense matrices.\"\"\"\n","  _transpose: 'Multiplier' = None\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    raise NotImplementedError()\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    raise NotImplementedError()\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    raise NotImplementedError()\n","\n","  def __matmul__(self, mat: tf.Tensor) -> tf.Tensor:\n","    tf.assert_equal(self.shape[1], shape(mat)[0])\n","    return self.matmul(mat)\n","\n","  def __rmatmul__(self, mat: tf.Tensor) -> tf.Tensor:\n","    tf.assert_equal(shape(mat)[-1], self.shape[0])\n","    return self.rmatmul(mat)\n","\n","  def __add__(self, mat: 'Multiplier') -> 'Multiplier':\n","    return Sum(self, mat)\n","\n","  def transpose(self) -> 'Multiplier':\n","    if self._transpose is None:\n","      self._transpose = Transpose(self)\n","    return self._transpose\n","\n","  def add_eye(self, diag_weight=float(1.0)) -> 'Multiplier':\n","    tf.assert_equal(self.shape[0], self.shape[1])\n","    return Sum(self, DiagMatrix(diag_weight * tf.ones([self.shape[0]])))\n","\n","  def rowsums(self, replace_if_0: 'None|float|tf.Tensor' = None) -> tf.Tensor:\n","    \"\"\"Returns vector with shape `num_rows = [self.shape[0]]` that sums rows.\n","\n","    Args:\n","      replace_if_0: If None, returns the actual sum, leaving zero-entries as-is.\n","        Otherwise, zero-entries will be replaced by this value.\n","    \"\"\"\n","    y = self @ tf.ones([self.shape[1]])  # M . 1\n","\n","    if replace_if_0 is not None:\n","      y = tf.where(tf.abs(y) < EPSILON, replace_if_0 * tf.ones_like(y), y)\n","    return y\n","\n","  def colsums(self, replace_if_0: 'None|float|tf.Tensor' = None) -> tf.Tensor:\n","    \"\"\"Returns vector with shape `num_cols = [self.shape[1]]` that sums columns.\n","\n","    Args:\n","      replace_if_0: If None, returns the actual sum, leaving zero-entries as-is.\n","        Otherwise, zero-entries will be replaced by this value.\n","    \"\"\"\n","    y = tf.ones([self.shape[0]]) @ self  # 1^T M  [shape=[cols]]\n","\n","    if replace_if_0 is not None:\n","      y = tf.where(tf.abs(y) < EPSILON, replace_if_0 * tf.ones_like(y), y)\n","    return y\n","\n","  def normalize_left(self) -> 'Multiplier':\n","    \"\"\"Returns a left-stochastic matrix.\"\"\"\n","    return Product(self, DiagMatrix(1 / self.colsums(1.0)))\n","\n","  def normalize_right(self) -> 'Multiplier':\n","    \"\"\"Returns a right-stochastic matrix.\"\"\"\n","    return Product(DiagMatrix(1 / self.rowsums(1.0)), self)\n","\n","  def normalize_leftright(self) -> 'Multiplier':\n","    return Product(\n","        DiagMatrix(tf.math.rsqrt(self.rowsums(1.0))),\n","        self,\n","        DiagMatrix(tf.math.rsqrt(self.colsums(1.0))),\n","    )\n","\n","  def normalize_symmetric(self) -> 'Multiplier':\n","    inv_sqrt_degree = DiagMatrix(tf.math.rsqrt(self.colsums(1.0)))\n","    return Product(inv_sqrt_degree, self, inv_sqrt_degree)\n","\n","\n","class Transpose(Multiplier):\n","  \"\"\"Defines matrix transpose.\"\"\"\n","\n","  def __init__(self, multiplier: Multiplier):\n","    self._multiplier = multiplier\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    # (M'X) == (X'M)'\n","    return tf.transpose(tf.transpose(mat) @ self._multiplier)\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    # (XM') == (XM')'' == (M X')'\n","    return tf.transpose(self._multiplier @ tf.transpose(mat))\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    transpose_shape = self._multiplier.shape\n","    return (transpose_shape[1], transpose_shape[0])\n","\n","  def transpose(self) -> Multiplier:\n","    return self._multiplier\n","\n","\n","class DiagMatrix(Multiplier):\n","  \"\"\"Defines diagonal matrix.\"\"\"\n","\n","  def __init__(self, diag_vector: tf.Tensor):\n","    assert len(diag_vector.shape) == 1, 'Must be a vector.'\n","    self._diag_vector = diag_vector\n","    self._vec_shape = shape(diag_vector)[0]\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.einsum('i,i...->i...', self._diag_vector, mat)\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.einsum('i,...i->...i', self._diag_vector, mat)\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return (self._vec_shape, self._vec_shape)\n","\n","\n","class Product(Multiplier):\n","  \"\"\"Defines product of multipliers.\"\"\"\n","\n","  def __init__(self, *multipliers: Multiplier):\n","    assert multipliers\n","    for i in range(1, len(multipliers)):\n","      tf.assert_equal(multipliers[i - 1].shape[1], multipliers[i].shape[0])\n","\n","    self._multipliers = multipliers\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    for m in self._multipliers[::-1]:\n","      mat = m @ mat\n","    return mat\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    for m in self._multipliers:\n","      mat = mat @ m\n","    return mat\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return (self._multipliers[0].shape[0], self._multipliers[-1].shape[1])\n","\n","\n","class Sum(Multiplier):\n","  \"\"\"Defines sum of multipliers.\"\"\"\n","\n","  def __init__(self, *multipliers: Multiplier):\n","    assert multipliers\n","    for i in range(1, len(multipliers)):\n","      tf.assert_equal(multipliers[i].shape[0], multipliers[0].shape[0])\n","      tf.assert_equal(multipliers[i].shape[1], multipliers[0].shape[1])\n","    self._multipliers = multipliers\n","\n","  def matmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.add_n([m @ mat for m in self._multipliers])\n","\n","  def rmatmul(self, mat: tf.Tensor) -> tf.Tensor:\n","    return tf.add_n([mat @ m for m in self._multipliers])\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    return self._multipliers[0].shape\n","\n","\n","class AdjacencyMultiplier(Multiplier):\n","  r\"\"\"Multiplies (sparse) adjacency with dense matrices.\n","\n","  Yields adjacency with (rows, cols) == (target, source).\n","\n","  `adj_multiplier @ x` yields tensor `y` with `y[i]` being `\\sum_{j->i} x[j]`.\n","\n","  Init Args:\n","      graph:\n","      sender_tag: If `== tfgnn.SOURCE`, then the (implicit) adjacency will be\n","        of shape `size_target x size_source`. If `== tfgnn.TARGET`, then `shape`\n","        should be `size_source x size_target`.\n","  \"\"\"\n","\n","  def __init__(\n","      self, graph, edge_set_name: tfgnn.EdgeSetName,\n","      edge_weight_feature_name: 'None|tfgnn.FieldName' = None,\n","      sender_tag: tfgnn.IncidentNodeTag = tfgnn.SOURCE):\n","    tfgnn.check_scalar_graph_tensor(graph, 'AdjacencyMultiplier')\n","    self._sender_tag = sender_tag\n","    self._receiver_tag: tfgnn.IncidentNodeTag = 1 - sender_tag\n","    self._edge_set_name = edge_set_name\n","    self._graph = graph\n","    self._edge_weight_feature_name = edge_weight_feature_name\n","\n","  @property\n","  def shape(self) -> tuple['int|tf.Tensor', 'int|tf.Tensor']:\n","    \"\"\"Shape is (size of receiver node set, size of sender node set).\"\"\"\n","    adj = self._graph.edge_sets[self._edge_set_name].adjacency\n","    sender_node_set_name = adj.node_set_name(self._sender_tag)\n","    receiver_node_set_name = adj.node_set_name(self._receiver_tag)\n","    sender_sizes = self._graph.node_sets[sender_node_set_name].sizes\n","    receiver_sizes = self._graph.node_sets[receiver_node_set_name].sizes\n","    return (tf.cast(tf.reduce_sum(receiver_sizes), tf.int32),\n","            tf.cast(tf.reduce_sum(sender_sizes), tf.int32))\n","\n","  def matmul(self, mat: tf.Tensor):\n","    edge_level = tfgnn.broadcast_node_to_edges(\n","        self._graph, self._edge_set_name, self._sender_tag, feature_value=mat)\n","\n","    if self._edge_weight_feature_name:\n","      edge_set = self._graph.edge_sets[self._edge_set_name]\n","      edge_level *= edge_set[self._edge_weight_feature_name]\n","\n","    return tfgnn.pool_edges_to_node(\n","        self._graph, self._edge_set_name, self._receiver_tag,\n","        feature_value=edge_level)\n","\n","  def rmatmul(self, mat):\n","    edge_level = tfgnn.broadcast_node_to_edges(\n","        self._graph, self._edge_set_name, self._receiver_tag,\n","        feature_value=tf.transpose(mat))\n","\n","    if self._edge_weight_feature_name:\n","      edge_set = self._graph.edge_sets[self._edge_set_name]\n","      edge_level *= edge_set[self._edge_weight_feature_name]\n","\n","    return tf.transpose(tfgnn.pool_edges_to_node(\n","        self._graph, self._edge_set_name, self._sender_tag,\n","        feature_value=edge_level))\n","\n","\n","def shape(tensor: tf.Tensor) -> 'list[int]|tf.Tensor':\n","  \"\"\"Helper function returns shape of eager or symbolic tensors.\"\"\"\n","  if any([s is None for s in tensor.shape]):\n","    return tf.shape(tensor)\n","  else:\n","    return tensor.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.446779Z","iopub.status.busy":"2023-11-16T23:52:27.445750Z","iopub.status.idle":"2023-11-16T23:52:27.600909Z","shell.execute_reply":"2023-11-16T23:52:27.600011Z","shell.execute_reply.started":"2023-11-16T23:52:27.446742Z"},"id":"u690dfq_hVqs","trusted":true},"outputs":[],"source":["###TRAINING\n","# Install standard modules\n","class _OpEmbedding(tf.keras.Model):\n","  \"\"\"Embeds GraphTensor.node_sets['op']['op'] nodes into feature 'op_e'.\"\"\"\n","\n","  def __init__(self, num_ops: int, embed_d: int, l2reg: float = 1e-4):\n","    super().__init__()\n","    self.embedding_layer = tf.keras.layers.Embedding(\n","        num_ops, embed_d, activity_regularizer=tf.keras.regularizers.l2(l2reg))\n","\n","  def call(\n","      self, graph: tfgnn.GraphTensor,\n","      training: bool = False) -> tfgnn.GraphTensor:\n","    op_features = dict(graph.node_sets['op'].features)\n","    op_features['op_e'] = self.embedding_layer(\n","        tf.cast(graph.node_sets['op']['op'], tf.int32))\n","    return graph.replace_features(node_sets={'op': op_features})\n","\n","\n","def pair_layout_graph_with_label(graph: tfgnn.GraphTensor):\n","    \"\"\"Extracts label from graph (`tfgnn.GraphTensor`) and returns a pair of `(graph, label)`\"\"\"\n","    # Return runtimes divded over large number: only ranking is required. The\n","    # runtimes are in the 100K range\n","    label = tf.cast(graph.node_sets['g']['runtimes'], tf.float32) / 1e7\n","    return graph, label\n","\n","\n","\n","class ResModel(tf.keras.Model):\n","    \"\"\"GNN with residual connections.\"\"\"\n","\n","    def __init__(\n","        self, num_configs: int, num_ops: int, op_embed_dim: int = 32,\n","        num_gnns: int = 2, mlp_layers: int = 2,\n","        hidden_activation: str = 'leaky_relu',\n","        hidden_dim: int = 32, reduction: str = 'sum'):\n","        super().__init__()\n","        self._num_configs = num_configs\n","        self._num_ops = num_ops\n","        self._op_embedding = _OpEmbedding(num_ops, op_embed_dim)\n","        self._prenet = _mlp([hidden_dim] * mlp_layers, hidden_activation)\n","        self._gc_layers = []\n","        for _ in range(num_gnns):\n","            self._gc_layers.append(_mlp([hidden_dim] * mlp_layers, hidden_activation))\n","        self._postnet = _mlp([hidden_dim, 1], hidden_activation, use_bias=False)\n","\n","    def call(self, graph: tfgnn.GraphTensor, training: bool = False):\n","        del training\n","        return self.forward(graph, self._num_configs)\n","\n","    def _node_level_forward(\n","        self, node_features: tf.Tensor,\n","        config_features: tf.Tensor,\n","        graph: tfgnn.GraphTensor, num_configs: int,\n","        edgeset_prefix='') -> tf.Tensor:\n","        \"\"\"implements the full computation within a GNN layer:\n","        obtains adjacency Matrices and normalizes them, \n","        transforms and normalizes nodes and configuration.\n","        applies the Pre-processing MLP and performs the Graph Convolution Operation.\n","        \"\"\"\n","    \n","        adj_op_op = AdjacencyMultiplier(\n","            graph, edgeset_prefix+'feed')  # op->op\n","        adj_config = AdjacencyMultiplier(\n","            graph, edgeset_prefix+'config')  # nconfig->op\n","\n","        adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n","        adj_op_op_hat = adj_op_op_hat.normalize_symmetric()\n","\n","        x = node_features\n","\n","        x = tf.stack([x] * num_configs, axis=1)\n","        config_features = 100 * (adj_config @ config_features)\n","        x = tf.concat([config_features, x], axis=-1)\n","        x = self._prenet(x)\n","        x = tf.nn.leaky_relu(x)\n","\n","        for layer in self._gc_layers:\n","            y = x\n","            y = tf.concat([config_features, y], axis=-1)\n","            y = tf.nn.leaky_relu(layer(adj_op_op_hat @ y))\n","            x += y\n","        return x\n","\n","    def forward(\n","        self, graph: tfgnn.GraphTensor, num_configs: int,\n","        backprop=True) -> tf.Tensor:\n","        \"\"\"\n","        Overall forward pass within the embedding layer,\n","        the node-level forward pass (_node_level_forward),\n","        and the final global pooling and post-processing stages.\n","        \"\"\"\n","        graph = self._op_embedding(graph)\n","\n","        config_features = graph.node_sets['nconfig']['feats']\n","        node_features = tf.concat([\n","            graph.node_sets['op']['feats'],\n","            graph.node_sets['op']['op_e']\n","        ], axis=-1)\n","\n","        x_full = self._node_level_forward(\n","            node_features=tf.stop_gradient(node_features),\n","            config_features=tf.stop_gradient(config_features),\n","            graph=graph, num_configs=num_configs)\n","\n","        if backprop:\n","            x_backprop = self._node_level_forward(\n","                node_features=node_features,\n","                config_features=config_features,\n","                graph=graph, num_configs=num_configs, edgeset_prefix='sampled_')\n","\n","            is_selected = graph.node_sets['op']['selected']\n","            # Need to expand twice as `is_selected` is a vector (num_nodes) but\n","            # x_{backprop, full} are 3D tensors (num_nodes, num_configs, num_feats).\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            x = tf.where(is_selected, x_backprop, x_full)\n","        else:\n","            x = x_full\n","\n","        adj_config = AdjacencyMultiplier(graph, 'config')\n","\n","        # Features for configurable nodes.\n","        config_feats = (adj_config.transpose() @ x)\n","\n","        # Global pooling\n","        adj_pool_op_sum = AdjacencyMultiplier(graph, 'g_op').transpose()\n","        adj_pool_op_mean = adj_pool_op_sum.normalize_right()\n","        adj_pool_config_sum = AdjacencyMultiplier(\n","            graph, 'g_config').transpose()\n","        x = self._postnet(tf.concat([\n","            # (A D^-1) @ Features\n","            adj_pool_op_mean @ x,\n","            # l2_normalize( A @ Features )\n","            tf.nn.l2_normalize(adj_pool_op_sum @ x, axis=-1),\n","            # l2_normalize( A @ Features )\n","            tf.nn.l2_normalize(adj_pool_config_sum @ config_feats, axis=-1),\n","        ], axis=-1))\n","\n","        x = tf.squeeze(x, -1)\n","\n","        return x\n","\n","def _mlp(dims, hidden_activation, l2reg=1e-4, use_bias=True):\n","  \"\"\"Helper function for multi-layer perceptron (MLP).\"\"\"\n","  layers = []\n","  for i, dim in enumerate(dims):\n","    if i > 0:\n","      layers.append(tf.keras.layers.Activation(hidden_activation))\n","    layers.append(tf.keras.layers.Dense(\n","        dim, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n","        use_bias=use_bias))\n","  return tf.keras.Sequential(layers)\n","\n","\"\"\"\n","CREATE DATASETS FOR TRAINING\n","\"\"\"\n","\n","def pull_data(CONFIGS_PER_GRAPH, MAX_TRAIN_CONFIGS, MAX_NUM_CONFIGS, MAX_KEEP_NODES, BATCH_SIZE, layout_data_root_dir):\n","  layout_npz_dataset = get_npz_dataset(\n","      layout_data_root_dir,\n","      min_train_configs=CONFIGS_PER_GRAPH,\n","      max_train_configs= MAX_NUM_CONFIGS,  # If any graph has more than this configurations, it will be filtered [speeds up loading + training]\n","      cache_dir='cache'\n","  )\n","\n","  layout_train_ds = (\n","        layout_npz_dataset.train.get_graph_tensors_dataset(\n","            config_samples = CONFIGS_PER_GRAPH,\n","            max_nodes=MAX_KEEP_NODES)\n","        .shuffle(100, reshuffle_each_iteration=True)\n","        .batch(BATCH_SIZE, drop_remainder=False)\n","        .map(tfgnn.GraphTensor.merge_batch_to_components)\n","        .map(pair_layout_graph_with_label))\n","\n","  layout_valid_ds = (\n","        layout_npz_dataset.validation.get_graph_tensors_dataset(\n","            config_samples = CONFIGS_PER_GRAPH)\n","        .batch(BATCH_SIZE, drop_remainder=False)\n","        .map(tfgnn.GraphTensor.merge_batch_to_components)\n","        .map(pair_layout_graph_with_label))\n","\n","  return layout_npz_dataset, layout_train_ds, layout_valid_ds\n","\n","\n","\n","def create_model(CONFIGS_PER_GRAPH, layout_npz_dataset):\n","  model = ResModel(CONFIGS_PER_GRAPH, layout_npz_dataset.num_ops)\n","\n","  loss = tfr.keras.losses.ListMLELoss()  # (temperature=10)\n","  opt = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=0.5)\n","\n","  model.compile(loss=loss, optimizer=opt, metrics=[\n","      tfr.keras.metrics.OPAMetric(name='opa_metric'),\n","  ])\n","\n","  return model\n","\n","def train_model(model, epochs, layout_train_ds, layout_valid_ds):\n","  best_val_opa = -1  # Tracks best validation OPA\n","  best_val_at_epoch = -1  # At which epoch.\n","\n","  for i in range(epochs):\n","      history = model.fit(\n","          layout_train_ds, epochs=1, verbose=1, validation_data=layout_valid_ds,\n","          validation_freq=1)\n","\n","      train_loss = history.history['loss'][-1]\n","      train_opa = history.history['opa_metric'][-1]\n","      val_loss = history.history['val_loss'][-1]\n","      val_opa = history.history['val_opa_metric'][-1]\n","      if val_opa > best_val_opa:\n","          best_val_opa = val_opa\n","          best_val_at_epoch = i\n","          best_params = {v.ref: v + 0 for v in model.trainable_variables}\n","          print(' * [@%i] Validation (NEW BEST): %s' % (i, str(val_opa)))\n","      elif early_stop > 0 and i - best_val_at_epoch >= early_stop:\n","        print('[@%i] Best accuracy was attained at epoch %i. Stopping.' % (i, best_val_at_epoch))\n","        break\n","  # Restore best parameters.\n","  print('Restoring parameters corresponding to the best validation OPA.')\n","  assert best_params is not None\n","  for v in model.trainable_variables:\n","      v.assign(best_params[v.ref])\n","\n","  return model, train_loss, train_opa, val_loss, val_opa, best_params\n","\n","\n","def run_inference(model, _INFERENCE_CONFIGS_BATCH_SIZE, layout_npz_dataset):\n","  print('\\n\\n   Running inference on test set ...\\n\\n')\n","  test_rankings = []\n","\n","  assert layout_npz_dataset.test.graph_id is not None\n","  for graph in tqdm.tqdm(layout_npz_dataset.test.iter_graph_tensors(),\n","                        total=layout_npz_dataset.test.graph_id.shape[-1],\n","                        desc='Inference'):\n","      # print(graph)\n","      num_configs = graph.node_sets['g']['runtimes'].shape[-1]\n","      # print(num_configs)\n","      # print(MAX_KEEP_NODES)\n","      # print(\"\\n\\n\\n\")\n","      all_scores = []\n","      for i in tqdm.tqdm(range(0, num_configs, _INFERENCE_CONFIGS_BATCH_SIZE)):\n","          end_i = min(i + _INFERENCE_CONFIGS_BATCH_SIZE, num_configs)\n","          # Take a cut of the configs.\n","          node_set_g = graph.node_sets['g']\n","          subconfigs_graph = tfgnn.GraphTensor.from_pieces(\n","              edge_sets=graph.edge_sets,\n","              node_sets={\n","                  'op': graph.node_sets['op'],\n","                  'nconfig': tfgnn.NodeSet.from_fields(\n","                      sizes=graph.node_sets['nconfig'].sizes,\n","                      features={\n","                          'feats': graph.node_sets['nconfig']['feats'][:, i:end_i],\n","                      }),\n","                  'g': tfgnn.NodeSet.from_fields(\n","                      sizes=tf.constant([1]),\n","                      features={\n","                          'graph_id': node_set_g['graph_id'],\n","                          'runtimes': node_set_g['runtimes'][:, i:end_i],\n","                          'kept_node_ratio': node_set_g['kept_node_ratio'],\n","                      })\n","              })\n","          h = model.forward(subconfigs_graph, num_configs=(end_i - i),\n","                            backprop=False)\n","          all_scores.append(h[0])\n","      all_scores = tf.concat(all_scores, axis=0)\n","      graph_id = graph.node_sets['g']['graph_id'][0].numpy().decode()\n","      sorted_indices = tf.strings.join(\n","          tf.strings.as_string(tf.argsort(all_scores)), ';').numpy().decode()\n","      test_rankings.append((graph_id, sorted_indices))\n","  return test_rankings\n","\n","def write_output(test_rankings, output_csv_filename, SOURCE, SEARCH):\n","    with tf.io.gfile.GFile(output_csv_filename, 'w') as fout:\n","        fout.write('ID,TopConfigs\\n')\n","        for graph_id, ranks in test_rankings:\n","            fout.write(f'layout:{SOURCE}:{SEARCH}:{graph_id},{ranks}\\n')\n","    print('\\n\\n   ***  Wrote', output_csv_filename, '\\n\\n')\n","\n","\"\"\"\n","BEGIN RUNNING CODE!!!\n","THERE ARE SETTINGS AND HYPERPARAMETERES\n","\"\"\"\n","\n","def main(source, search, **kwargs):\n","  # need to download npz\n","  # tile_data_ROOT = '/npz/layout'\n","  tile_data_ROOT = '/content/drive/MyDrive/npz 2/layout'\n","  SOURCE = source  # Can be \"xla\" or \"nlp\"\n","  SEARCH = search  # Can be \"random\" or \"default\"\n","\n","  tile_data_root_dir = os.path.join(\n","        os.path.expanduser(tile_data_ROOT), SOURCE, SEARCH)\n","\n","  # Batch size information.\n","  # BATCH_SIZE = 10  # Number of graphs per batch.\n","  # CONFIGS_PER_GRAPH = 2  # Number of configurations (features and target values) per graph.\n","  # MAX_NUM_CONFIGS = 20 # maximum number of configurations to filter for\n","  # MAX_KEEP_NODES = 100  # Useful for dropout.\n","  # MAX_TRAIN_CONFIGS = 20\n","\n","  BATCH_SIZE = kwargs['batch_size']  # Number of graphs per batch.\n","  CONFIGS_PER_GRAPH = kwargs['configs_per_graph']  # Number of configurations (features and target values) per graph.\n","  MAX_NUM_CONFIGS = kwargs['max_num_configs'] # maximum number of configurations to filter for\n","  MAX_KEEP_NODES = kwargs['max_keep_nodes']  # Useful for dropout.\n","  MAX_TRAIN_CONFIGS = kwargs['max_train_configs']\n","\n","  # edges \"sampled_config\" and \"sampled_feed\" (or, \"con50fig\" and \"feed\")\n","  early_stop = 5  # If validation OPA did not increase in this many epochs, terminate training.\n","  best_params = None  # Stores parameters corresponding to best validation OPA, to restore to them after training.\n","  epochs = 1  # Total number of training epochs.\n","\n","  # pull the data\n","  \n","  tiles_npz_dataset, tile_train_ds, tile_valid_ds = pull_data(CONFIGS_PER_GRAPH, MAX_KEEP_NODES, BATCH_SIZE, tile_data_root_dir)\n","  model = create_model(CONFIGS_PER_GRAPH, tiles_npz_dataset)\n","  model, train_loss, train_opa, val_loss, val_opa, best_params = train_model(model, epochs, tile_train_ds, tile_valid_ds)\n","\n","\n","  _INFERENCE_CONFIGS_BATCH_SIZE = 50\n","  # _INFERENCE_CONFIGS_BATCH_SIZE = 100\n","\n","  folder_path = '/content/drive/MyDrive/tpu_graphs/outputcsvs/'\n","  output_csv_filename = f'inference_layout_{SOURCE}_{SEARCH}.csv'\n","  output_csv_filename = folder_path + output_csv_filename\n","\n","  test_rankings = run_inference(model, _INFERENCE_CONFIGS_BATCH_SIZE, tiles_npz_dataset)\n","  write_output(test_rankings, output_csv_filename, SOURCE, SEARCH)\n"]},{"cell_type":"markdown","metadata":{"id":"0Lz-bLTSa8HH"},"source":["# Params to Configure\n","\n","For every Kaggle submission, the same hyperparameters need to be used four times, in order to produce 4 output CSVs. The 4 configurations are:\n","1. source = nlp; search = random\n","2. source = nlp; search = default\n","3. source = xla; search = random\n","4. source = xla; search = default\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.602156Z","iopub.status.busy":"2023-11-16T23:52:27.601880Z","iopub.status.idle":"2023-11-16T23:52:27.607044Z","shell.execute_reply":"2023-11-16T23:52:27.606025Z","shell.execute_reply.started":"2023-11-16T23:52:27.602132Z"},"id":"DDbaXawQNUNZ","trusted":true},"outputs":[],"source":["source = 'xla' # Has to be be \"xla\"\n","batch_size = 10  # Number of graphs per batch.\n","configs_per_graph = 2  # Number of configurations (features and target values) per graph.\n","max_num_configs = 20 # maximum number of configurations to filter for\n","max_keep_nodes = 100  # Useful for dropout.\n","max_train_configs = 20 # If any graph has more than this configurations, it will be filtered [speeds up loading + training]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-16T23:52:27.608503Z","iopub.status.busy":"2023-11-16T23:52:27.608176Z"},"executionInfo":{"elapsed":1181867,"status":"ok","timestamp":1700110797067,"user":{"displayName":"Valeria Vera Lagos","userId":"03700786808723630376"},"user_tz":300},"id":"0QOrsI3L4IEX","outputId":"031e610b-8ed6-4b2d-9bba-a28d2e5a7052","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["AAA\n","dataset cache file:  cache/da030f462f243e051c33e7eb1886d667-cache.npz\n","loaded from cache/da030f462f243e051c33e7eb1886d667-cache.npz\n","dataset cache file:  cache/4378df730c04529a89e472ddf67e9460-cache.npz\n","loaded from cache/4378df730c04529a89e472ddf67e9460-cache.npz\n","dataset cache file:  cache/66a2b3a0bac484e0aec26d531f74b257-cache.npz\n","loaded from cache/66a2b3a0bac484e0aec26d531f74b257-cache.npz\n","21/21 [==============================] - 205s 9s/step - loss: 0.7109 - opa_metric: 0.5556 - val_loss: 0.6971 - val_opa_metric: 0.6000\n"," * [@0] Validation (NEW BEST): 0.6000000238418579\n","Restoring parameters corresponding to the best validation OPA.\n","\n","\n","   Running inference on test set ...\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Inference:   0%|          | 0/17 [00:00<?, ?it/s]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:01<00:36,  1.90s/it]\u001b[A\n"," 10%|█         | 2/20 [00:03<00:32,  1.82s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:05<00:30,  1.80s/it]\u001b[A\n"," 20%|██        | 4/20 [00:07<00:28,  1.79s/it]\u001b[A\n"," 25%|██▌       | 5/20 [00:09<00:26,  1.79s/it]\u001b[A\n"," 30%|███       | 6/20 [00:10<00:25,  1.79s/it]\u001b[A\n"," 35%|███▌      | 7/20 [00:12<00:23,  1.78s/it]\u001b[A\n"," 40%|████      | 8/20 [00:14<00:21,  1.78s/it]\u001b[A\n"," 45%|████▌     | 9/20 [00:16<00:19,  1.77s/it]\u001b[A\n"," 50%|█████     | 10/20 [00:17<00:17,  1.78s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [00:19<00:15,  1.77s/it]\u001b[A\n"," 60%|██████    | 12/20 [00:21<00:14,  1.77s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [00:23<00:12,  1.77s/it]\u001b[A\n"," 70%|███████   | 14/20 [00:24<00:10,  1.77s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [00:26<00:08,  1.78s/it]\u001b[A\n"," 80%|████████  | 16/20 [00:28<00:07,  1.78s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [00:30<00:05,  1.77s/it]\u001b[A\n"," 90%|█████████ | 18/20 [00:32<00:03,  1.77s/it]\u001b[A\n"," 95%|█████████▌| 19/20 [00:33<00:01,  1.77s/it]\u001b[A\n","100%|██████████| 20/20 [00:35<00:00,  1.78s/it]\u001b[A\n","Inference:   6%|▌         | 1/17 [00:36<09:41, 36.33s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:06<02:03,  6.48s/it]\u001b[A\n"," 10%|█         | 2/20 [00:12<01:56,  6.45s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:19<01:49,  6.41s/it]\u001b[A\n"," 20%|██        | 4/20 [00:25<01:42,  6.40s/it]\u001b[A\n"," 25%|██▌       | 5/20 [00:32<01:36,  6.41s/it]\u001b[A\n"," 30%|███       | 6/20 [00:38<01:29,  6.43s/it]\u001b[A\n"," 35%|███▌      | 7/20 [00:44<01:23,  6.42s/it]\u001b[A\n"," 40%|████      | 8/20 [00:51<01:17,  6.42s/it]\u001b[A\n"," 45%|████▌     | 9/20 [00:57<01:10,  6.40s/it]\u001b[A\n"," 50%|█████     | 10/20 [01:04<01:04,  6.41s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [01:10<00:57,  6.42s/it]\u001b[A\n"," 60%|██████    | 12/20 [01:16<00:51,  6.40s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [01:23<00:44,  6.41s/it]\u001b[A\n"," 70%|███████   | 14/20 [01:29<00:38,  6.42s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [01:36<00:32,  6.45s/it]\u001b[A\n"," 80%|████████  | 16/20 [01:42<00:25,  6.45s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [01:49<00:19,  6.43s/it]\u001b[A\n"," 90%|█████████ | 18/20 [01:55<00:12,  6.44s/it]\u001b[A\n"," 95%|█████████▌| 19/20 [02:02<00:06,  6.44s/it]\u001b[A\n","100%|██████████| 20/20 [02:08<00:00,  6.43s/it]\u001b[A\n","Inference:  12%|█▏        | 2/17 [02:45<22:43, 90.92s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:04<01:18,  4.13s/it]\u001b[A\n"," 10%|█         | 2/20 [00:08<01:13,  4.06s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:12<01:08,  4.05s/it]\u001b[A\n"," 20%|██        | 4/20 [00:16<01:04,  4.04s/it]\u001b[A\n"," 25%|██▌       | 5/20 [00:20<01:00,  4.03s/it]\u001b[A\n"," 30%|███       | 6/20 [00:24<00:56,  4.04s/it]\u001b[A\n"," 35%|███▌      | 7/20 [00:28<00:52,  4.06s/it]\u001b[A\n"," 40%|████      | 8/20 [00:32<00:49,  4.09s/it]\u001b[A\n"," 45%|████▌     | 9/20 [00:36<00:44,  4.09s/it]\u001b[A\n"," 50%|█████     | 10/20 [00:40<00:40,  4.09s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [00:44<00:36,  4.07s/it]\u001b[A\n"," 60%|██████    | 12/20 [00:48<00:32,  4.08s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [00:53<00:28,  4.12s/it]\u001b[A\n"," 70%|███████   | 14/20 [00:57<00:24,  4.11s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [01:01<00:20,  4.10s/it]\u001b[A\n"," 80%|████████  | 16/20 [01:05<00:16,  4.08s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [01:09<00:12,  4.07s/it]\u001b[A\n"," 90%|█████████ | 18/20 [01:13<00:08,  4.06s/it]\u001b[A\n"," 95%|█████████▌| 19/20 [01:17<00:04,  4.05s/it]\u001b[A\n","100%|██████████| 20/20 [01:21<00:00,  4.07s/it]\u001b[A\n","Inference:  18%|█▊        | 3/17 [04:07<20:16, 86.87s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:14<04:41, 14.82s/it]\u001b[A\n"," 10%|█         | 2/20 [00:29<04:25, 14.78s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:44<04:11, 14.81s/it]\u001b[A\n"," 20%|██        | 4/20 [00:59<03:56, 14.79s/it]\u001b[A\n"," 25%|██▌       | 5/20 [01:14<03:42, 14.81s/it]\u001b[A\n"," 30%|███       | 6/20 [01:28<03:26, 14.78s/it]\u001b[A\n"," 35%|███▌      | 7/20 [01:43<03:12, 14.79s/it]\u001b[A\n"," 40%|████      | 8/20 [01:58<02:57, 14.77s/it]\u001b[A\n"," 45%|████▌     | 9/20 [02:13<02:42, 14.79s/it]\u001b[A\n"," 50%|█████     | 10/20 [02:27<02:27, 14.76s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [02:42<02:13, 14.78s/it]\u001b[A\n"," 60%|██████    | 12/20 [02:57<01:58, 14.76s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [03:12<01:43, 14.75s/it]\u001b[A\n"," 70%|███████   | 14/20 [03:26<01:28, 14.77s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [03:41<01:13, 14.78s/it]\u001b[A\n"," 80%|████████  | 16/20 [03:56<00:59, 14.77s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [04:11<00:44, 14.78s/it]\u001b[A\n"," 90%|█████████ | 18/20 [04:26<00:29, 14.78s/it]\u001b[A\n"," 95%|█████████▌| 19/20 [04:40<00:14, 14.78s/it]\u001b[A\n","100%|██████████| 20/20 [04:55<00:00, 14.78s/it]\u001b[A\n","Inference:  24%|██▎       | 4/17 [09:03<36:43, 169.49s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:02<00:43,  2.29s/it]\u001b[A\n"," 10%|█         | 2/20 [00:04<00:41,  2.30s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:06<00:39,  2.30s/it]\u001b[A\n"," 20%|██        | 4/20 [00:09<00:36,  2.29s/it]\u001b[A\n"," 25%|██▌       | 5/20 [00:11<00:34,  2.28s/it]\u001b[A\n"," 30%|███       | 6/20 [00:13<00:32,  2.30s/it]\u001b[A\n"," 35%|███▌      | 7/20 [00:16<00:29,  2.29s/it]\u001b[A\n"," 40%|████      | 8/20 [00:18<00:27,  2.28s/it]\u001b[A\n"," 45%|████▌     | 9/20 [00:20<00:25,  2.28s/it]\u001b[A\n"," 50%|█████     | 10/20 [00:22<00:22,  2.29s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [00:25<00:20,  2.28s/it]\u001b[A\n"," 60%|██████    | 12/20 [00:27<00:18,  2.28s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [00:29<00:15,  2.27s/it]\u001b[A\n"," 70%|███████   | 14/20 [00:31<00:13,  2.27s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [00:34<00:11,  2.28s/it]\u001b[A\n"," 80%|████████  | 16/20 [00:36<00:09,  2.27s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [00:38<00:06,  2.27s/it]\u001b[A\n"," 90%|█████████ | 18/20 [00:41<00:04,  2.27s/it]\u001b[A\n"," 95%|█████████▌| 19/20 [00:43<00:02,  2.28s/it]\u001b[A\n","100%|██████████| 20/20 [00:45<00:00,  2.28s/it]\u001b[A\n","Inference:  29%|██▉       | 5/17 [09:49<25:00, 125.04s/it]\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n","  5%|▌         | 1/20 [00:01<00:35,  1.88s/it]\u001b[A\n"," 10%|█         | 2/20 [00:03<00:33,  1.89s/it]\u001b[A\n"," 15%|█▌        | 3/20 [00:05<00:32,  1.91s/it]\u001b[A\n"," 20%|██        | 4/20 [00:07<00:30,  1.91s/it]\u001b[A\n"," 25%|██▌       | 5/20 [00:09<00:28,  1.90s/it]\u001b[A\n"," 30%|███       | 6/20 [00:11<00:26,  1.90s/it]\u001b[A\n"," 35%|███▌      | 7/20 [00:13<00:24,  1.90s/it]\u001b[A\n"," 40%|████      | 8/20 [00:15<00:22,  1.90s/it]\u001b[A\n"," 45%|████▌     | 9/20 [00:17<00:20,  1.91s/it]\u001b[A\n"," 50%|█████     | 10/20 [00:19<00:19,  1.91s/it]\u001b[A\n"," 55%|█████▌    | 11/20 [00:20<00:17,  1.91s/it]\u001b[A\n"," 60%|██████    | 12/20 [00:22<00:15,  1.91s/it]\u001b[A\n"," 65%|██████▌   | 13/20 [00:24<00:13,  1.90s/it]\u001b[A\n"," 70%|███████   | 14/20 [00:26<00:11,  1.91s/it]\u001b[A\n"," 75%|███████▌  | 15/20 [00:28<00:09,  1.92s/it]\u001b[A\n"," 80%|████████  | 16/20 [00:30<00:07,  1.91s/it]\u001b[A\n"," 85%|████████▌ | 17/20 [00:32<00:05,  1.91s/it]\u001b[A\n"," 90%|█████████ | 18/20 [00:34<00:03,  1.90s/it]\u001b[A"]}],"source":["main(source = source,\n","     search = '',\n","      batch_size = batch_size,\n","     configs_per_graph = configs_per_graph,\n","     max_num_configs = max_num_configs,\n","     max_keep_nodes = max_keep_nodes,\n","     max_train_configs = max_train_configs\n","     )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6Ag2Dlv4mxV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6641124,"sourceId":58266,"sourceType":"competition"},{"datasetId":4014081,"sourceId":6984420,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
